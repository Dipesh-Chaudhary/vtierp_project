{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VisioTextual Insight Engine for Research Papers (VTIERP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VTIERP: Exploring Multimodal RAG for Enhanced Research Paper Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Research papers are rich sources of information, presenting findings not only through text but also critically through visual elements like figures, diagrams, and tables. Traditional RAG (Retrieval Augmented Generation) systems often focus solely on textual content, potentially missing crucial insights conveyed visually. This notebook documents the exploration and development of a VisioTextual Insight Engine for Research Papers (VTIERP), aiming to create a more holistic understanding by integrating information from both textual and visual modalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Problem Statement\n",
    "\n",
    "How can we effectively extract, represent, and query information from both the textual content and the visual elements (and their descriptions/captions) within research PDFs to answer complex user queries that may require cross-modal understanding?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Objectives\n",
    "\n",
    "*   Explore methods for robust text extraction from PDFs.\n",
    "*   Investigate techniques for identifying and extracting visual elements (images, diagrams, tables).\n",
    "*   Develop a strategy for generating meaningful textual descriptions for these visual elements, especially when explicit captions are missing or insufficient (simulating advanced VLM capabilities).\n",
    "*   Design a RAG pipeline that can leverage both textual chunks and visual element descriptions.\n",
    "*   Evaluate the approach with sample research papers and complex queries.\n",
    "*   Utilize Langchain and LangGraph for building the RAG system and agentic control flow.\n",
    "*   Employ Google Gemini models for embedding, auxiliary tasks (like summarization or simulated VLM descriptions), and final answer generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini RAG: models/gemini-2.5-flash-preview-05-20, Aux: models/gemini-2.5-flash-preview-05-20, Embeddings: models/text-embedding-004 initialized.\n",
      "Setup Complete.\n"
     ]
    }
   ],
   "source": [
    "# Essential imports, confuguration and also initialization of models and directories for the advanced version of the script\n",
    "import os\n",
    "import shutil\n",
    "import uuid\n",
    "import io\n",
    "import re\n",
    "import math # For get_pymupdf_drawing_clusters\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm # For progress bars if you like\n",
    "from collections import defaultdict # For caption_to_visual_parts_map\n",
    "import base64 # For image_to_base64\n",
    "\n",
    "import fitz # PyMuPDF\n",
    "\n",
    "# Langchain and related imports\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from typing import List, Dict, Tuple, Optional, Any, Set # For type hinting\n",
    "\n",
    "# Unstructured (optional, for cleaning/OCR)\n",
    "try:\n",
    "    from unstructured.cleaners.core import clean as unstruct_clean, clean_extra_whitespace as unstruct_clean_ws\n",
    "    UNSTRUCTURED_AVAILABLE = True\n",
    "except ImportError:\n",
    "    UNSTRUCTURED_AVAILABLE = False\n",
    "    print(\"Warning: Unstructured.io not found. Basic text cleaning will be used. OCR features might be limited.\")\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "NOTEBOOK_DIR = os.getcwd() # Important for path joining\n",
    "\n",
    "# API Key (ensure this is set)\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    from getpass import getpass\n",
    "    GOOGLE_API_KEY = getpass(\"Enter Google API Key: \")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "\n",
    "# PDF Files\n",
    "PDF_PATHS = [\"AAG.pdf\", \"Self_rewarding_VLLM.pdf\"] # Ensure these files are in NOTEBOOK_DIR or provide full paths\n",
    "\n",
    "# Output Directories for Exploration\n",
    "EXPLORATION_OUTPUT_BASE = os.path.join(NOTEBOOK_DIR, \"exploration_outputs\") # New versioned output\n",
    "IMAGE_SAVE_PARENT_DIR = os.path.join(EXPLORATION_OUTPUT_BASE, \"extracted_elements\")\n",
    "CHROMA_TEXT_PATH_NOTEBOOK = os.path.join(EXPLORATION_OUTPUT_BASE, \"chroma_db_text\")\n",
    "CHROMA_IMAGE_DESC_PATH_NOTEBOOK = os.path.join(EXPLORATION_OUTPUT_BASE, \"chroma_db_img_desc\")\n",
    "\n",
    "# Clean up previous exploration run outputs\n",
    "if os.path.exists(EXPLORATION_OUTPUT_BASE):\n",
    "    shutil.rmtree(EXPLORATION_OUTPUT_BASE)\n",
    "os.makedirs(IMAGE_SAVE_PARENT_DIR, exist_ok=True)\n",
    "os.makedirs(CHROMA_TEXT_PATH_NOTEBOOK, exist_ok=True)\n",
    "os.makedirs(CHROMA_IMAGE_DESC_PATH_NOTEBOOK, exist_ok=True)\n",
    "\n",
    "\n",
    "# LLM/VLM Configuration from your notebook (or simplified for exploration)\n",
    "MAX_IMAGES_TO_LLM_FINAL = 3\n",
    "MAX_ELEMENTS_FOR_VLM_DESCRIPTION_PER_PDF_EXPLORE = 3 # Keep low for notebook speed\n",
    "RENDER_DPI_PYMUPDF = 150\n",
    "MIN_VISUAL_WIDTH_PYMUPDF = 30\n",
    "MIN_VISUAL_HEIGHT_PYMUPDF = 30\n",
    "TEXT_BLOCK_MIN_AREA_FOR_OBSTRUCTION = 100\n",
    "DRAWING_CLUSTER_MAX_DIST_FACTOR = 0.03\n",
    "OCR_DPI = 300\n",
    "MIN_OCR_TEXT_LENGTH_FOR_SCANNED_PDF = 100\n",
    "\n",
    "# Initialize LLMs and Embeddings\n",
    "llm_rag = None\n",
    "llm_aux = None # This will be used for VLM-like descriptions\n",
    "embeddings = None\n",
    "try:\n",
    "    # Match models from your original notebook if possible and available\n",
    "    llm_rag = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-preview-05-20\", google_api_key=GOOGLE_API_KEY, temperature=0.0, max_output_tokens=8192)\n",
    "    llm_aux = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-preview-05-20\", google_api_key=GOOGLE_API_KEY, temperature=0.1, max_output_tokens=4096) # Ensure this model can handle image inputs if you test that part early\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\", google_api_key=GOOGLE_API_KEY)\n",
    "    print(f\"Gemini RAG: {llm_rag.model}, Aux: {llm_aux.model}, Embeddings: {embeddings.model} initialized.\")\n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL Error initializing Gemini models/embeddings: {e}. Notebook execution might fail.\")\n",
    "    # You might want to raise e here or handle it gracefully\n",
    "\n",
    "print(\"Setup Complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Acquisition & Preparation\n",
    "\n",
    "For this exploration, we will use the following publicly available research papers:\n",
    "*   `AAG.pdf` (Analogy-Augmented Generation)\n",
    "*   `Self_rewarding_VLLM.pdf` (Self-Rewarding Vision Language Models)\n",
    "\n",
    "These papers contain a good mix of text, complex diagrams, and tables, making them suitable for testing our multimodal approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Core Utility and Helper Functions\n",
    "\n",
    "Before we begin the extraction process, let's define the core helper functions that will be used throughout this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1: Text Processing and Image Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility functions defined.\n"
     ]
    }
   ],
   "source": [
    "def clean_parsed_text(text: str) -> str:\n",
    "    \"\"\"Cleans text by removing extra whitespace and handling ligatures.\"\"\"\n",
    "    if not text: return \"\"\n",
    "    if UNSTRUCTURED_AVAILABLE:\n",
    "        try:\n",
    "            text = unstruct_clean(text, bullets=False, extra_whitespace=False, dashes=False, trailing_punctuation=False)\n",
    "            text = unstruct_clean_ws(text)\n",
    "        except Exception: # Fallback\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    else:\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = text.replace(\"ﬁ\", \"fi\").replace(\"ﬂ\", \"fl\")\n",
    "    return text.strip()\n",
    "\n",
    "def image_to_base64(image_path: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"Converts an image file to a base64 string and returns its MIME type.\"\"\"\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Warning: Image path does not exist: {image_path}\")\n",
    "        return None, None\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            fmt = img.format.lower() if img.format else 'png'\n",
    "            save_format = fmt if fmt in [\"jpeg\", \"png\", \"gif\", \"webp\"] else \"PNG\"\n",
    "            if save_format.upper() == \"JPEG\" and img.mode == \"RGBA\": img = img.convert(\"RGB\")\n",
    "            bio = io.BytesIO()\n",
    "            img.save(bio, format=save_format)\n",
    "            img_bytes = bio.getvalue()\n",
    "            mime_type = f\"image/{save_format.lower()}\"\n",
    "            if save_format.upper() == \"JPEG\": mime_type = \"image/jpeg\"\n",
    "        return base64.b64encode(img_bytes).decode('utf-8'), mime_type\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting image to base64 for {os.path.basename(image_path)}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Global counter for VLM descriptions for this notebook run\n",
    "images_described_this_run_count = 0\n",
    "\n",
    "def generate_detailed_image_description(\n",
    "    image_path: str,\n",
    "    element_type: str = \"visual element\",\n",
    "    # llm_vlm: ChatGoogleGenerativeAI = llm_aux # Use the globally defined llm_aux\n",
    "    max_vlm_calls_limit: int = MAX_ELEMENTS_FOR_VLM_DESCRIPTION_PER_PDF_EXPLORE\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generates a detailed description of an image using a multimodal LLM (VLM).\n",
    "    Uses the global images_described_this_run_count and llm_aux.\n",
    "    \"\"\"\n",
    "    global images_described_this_run_count # To modify the global counter\n",
    "    if not llm_aux: return f\"VLM (aux_llm) not initialized for {os.path.basename(image_path)}.\"\n",
    "\n",
    "    if images_described_this_run_count >= max_vlm_calls_limit:\n",
    "        # print(f\"VLM description limit ({max_vlm_calls_limit}) reached for this run. Skipping VLM for {os.path.basename(image_path)}\")\n",
    "        return f\"Visual Element ({element_type}): {os.path.basename(image_path)}. VLM description skipped due to limit.\"\n",
    "\n",
    "    try:\n",
    "        img_base64, mime_type = image_to_base64(image_path)\n",
    "        if not img_base64: return f\"Could not load image {os.path.basename(image_path)} for VLM.\"\n",
    "\n",
    "        prompt_text = (f\"Expert document analyst: Analyze this {element_type} from a research paper. Describe its key visual components, structure, any text present within it, and its apparent purpose or the information it conveys. \"\n",
    "                       f\"For Tables: describe columns, data types, and notable trends. For Diagrams/Charts: describe type, axes, trends, and flow. Provide a concise, comprehensive summary suitable for Q&A.\")\n",
    "\n",
    "        message = HumanMessage(content=[\n",
    "            {\"type\": \"text\", \"text\": prompt_text},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:{mime_type};base64,{img_base64}\"}}\n",
    "        ])\n",
    "\n",
    "        response = llm_aux.invoke([message]) # Use the global llm_aux\n",
    "        images_described_this_run_count += 1\n",
    "        return clean_parsed_text(response.content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating VLM description for {os.path.basename(image_path)}: {e}\")\n",
    "        return f\"Error in VLM description for {os.path.basename(image_path)}.\"\n",
    "\n",
    "def ocr_image_to_text(image_path: str) -> str:\n",
    "    \"\"\"Performs OCR on an image file using Unstructured.io's partition.auto if available.\"\"\"\n",
    "    if not UNSTRUCTURED_AVAILABLE:\n",
    "        # print(\"Unstructured.io not available for OCR.\")\n",
    "        return \"\" # Return empty string if not available to avoid breaking flow\n",
    "    try:\n",
    "        from unstructured.partition.auto import partition # Local import\n",
    "        elements = partition(filename=image_path, strategy=\"ocr_only\") # or \"hi_res\" with model\n",
    "        full_text = \"\\n\".join([str(el.text) for el in elements if hasattr(el, 'text')])\n",
    "        return clean_parsed_text(full_text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during OCR for {os.path.basename(image_path)}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "print(\"Utility functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2: PyMuPDF Element Extraction Helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyMuPDF element extraction helpers defined.\n"
     ]
    }
   ],
   "source": [
    "def find_pymupdf_captions(page: fitz.Page) -> List[Dict]:\n",
    "    \"\"\"Extracts potential figure and table captions from a PyMuPDF page.\"\"\"\n",
    "    # (code for find_pymupdf_captions from the notebook)\n",
    "    captions = []\n",
    "    blocks = page.get_text(\"dict\", flags=fitz.TEXTFLAGS_DICT & ~fitz.TEXT_PRESERVE_LIGATURES & ~fitz.TEXT_PRESERVE_IMAGES)[\"blocks\"]\n",
    "    for block in blocks:\n",
    "        if block['type'] == 0:\n",
    "            block_text_content = \"\".join(span['text'] for line in block['lines'] for span in line['spans'])\n",
    "            cleaned_block_text = clean_parsed_text(block_text_content)\n",
    "            match = re.match(r\"^(Figure|Fig\\.?|Table)\\s+([A-Za-z0-9]+\\.?\\d*)\\s*[:\\.]?\\s*\", cleaned_block_text, re.IGNORECASE)\n",
    "            if match:\n",
    "                caption_type = \"figure\" if \"fig\" in match.group(1).lower() else \"table\"\n",
    "                caption_id = f\"{caption_type.capitalize()}{match.group(2)}\"\n",
    "                captions.append({\n",
    "                    \"text\": cleaned_block_text, \"bbox\": fitz.Rect(block['bbox']),\n",
    "                    \"type\": caption_type, \"page_num\": page.number, \"id\": caption_id\n",
    "                })\n",
    "    return captions\n",
    "\n",
    "def get_pymupdf_drawing_clusters(page: fitz.Page, max_dist_factor: float = DRAWING_CLUSTER_MAX_DIST_FACTOR) -> List[fitz.Rect]:\n",
    "    \"\"\"Clusters PyMuPDF drawing primitives into larger bounding boxes.\"\"\"\n",
    "    # (code for get_pymupdf_drawing_clusters from the notebook)\n",
    "    drawings = page.get_drawings()\n",
    "    path_rects = [d['rect'] for d in drawings if d['rect'].width > 1 and d['rect'].height > 1 and (d.get('type') != 'fill' or d.get('color') is not None) ]\n",
    "    if not path_rects: return []\n",
    "    merged_rects = []\n",
    "    path_rects.sort(key=lambda r: (r.y0, r.x0))\n",
    "    page_diag = math.sqrt(page.rect.width**2 + page.rect.height**2) if page.rect.width > 0 and page.rect.height > 0 else 1000\n",
    "    max_dist = page_diag * max_dist_factor\n",
    "    for r_obj in path_rects:\n",
    "        r_rect = fitz.Rect(r_obj)\n",
    "        if not merged_rects: merged_rects.append(r_rect)\n",
    "        else:\n",
    "            last = merged_rects[-1]\n",
    "            expanded_last = fitz.Rect(last) + (-max_dist, -max_dist, max_dist, max_dist)\n",
    "            if r_rect.intersects(expanded_last): merged_rects[-1] = last | r_rect\n",
    "            else: merged_rects.append(r_rect)\n",
    "    return [r_item for r_item in merged_rects if r_item.width > MIN_VISUAL_WIDTH_PYMUPDF / 2 and r_item.height > MIN_VISUAL_HEIGHT_PYMUPDF / 2]\n",
    "\n",
    "\n",
    "def find_table_content(page: fitz.Page, table_caption_bbox: fitz.Rect) -> str:\n",
    "    \"\"\"Attempts to extract text content of a textual table based on its caption's bounding box.\"\"\"\n",
    "    # (Your exact code for find_table_content from the notebook)\n",
    "    # This is a simplified example; use your more robust version\n",
    "    table_content_str = \"\"\n",
    "    search_rect = fitz.Rect(page.rect.x0, table_caption_bbox.y1, page.rect.x1, table_caption_bbox.y1 + 300)\n",
    "    text_blocks_in_roi = [\n",
    "        block for block in page.get_text(\"blocks\", flags=0)\n",
    "        if fitz.Rect(block[:4]).intersects(search_rect) and block[6] == 0\n",
    "    ]\n",
    "    text_blocks_in_roi.sort(key=lambda b: (b[1], b[0]))\n",
    "    potential_table_lines = []\n",
    "    last_table_line_y1 = table_caption_bbox.y1\n",
    "    for block in text_blocks_in_roi:\n",
    "        block_bbox = fitz.Rect(block[:4])\n",
    "        if block_bbox.y0 > last_table_line_y1 - 5 and block_bbox.y0 < last_table_line_y1 + 70:\n",
    "            caption_x_mid = (table_caption_bbox.x0 + table_caption_bbox.x1) / 2\n",
    "            block_x_mid = (block_bbox.x0 + block_bbox.x1) / 2\n",
    "            if abs(caption_x_mid - block_x_mid) < page.rect.width * 0.4: # Wider tolerance for exploration\n",
    "                block_text_content = page.get_text(clip=block_bbox, sort=True)\n",
    "                lines = [line.strip() for line in block_text_content.split('\\n') if len(line.strip()) > 3 and not re.match(r\"^\\d+$\", line.strip())]\n",
    "                if lines:\n",
    "                    potential_table_lines.extend(lines)\n",
    "                    last_table_line_y1 = max(last_table_line_y1, block_bbox.y1)\n",
    "        elif block_bbox.y0 >= last_table_line_y1 + 70:\n",
    "            break\n",
    "    if potential_table_lines: table_content_str = \"\\n\".join(potential_table_lines)\n",
    "    return clean_parsed_text(table_content_str) if table_content_str else \"\"\n",
    "\n",
    "\n",
    "def refine_roi_by_content_and_text(page: fitz.Page, initial_roi: fitz.Rect, is_likely_table: bool) -> Optional[fitz.Rect]:\n",
    "    \"\"\"Refines the bounding box (ROI) of a visual element.\"\"\"\n",
    "    # (Your exact code for refine_roi_by_content_and_text from the notebook)\n",
    "    roi = fitz.Rect(initial_roi); page_rect = page.rect\n",
    "    if not roi.intersects(page_rect) or roi.is_empty: return None\n",
    "    content_bbox = fitz.Rect(); has_explicit_visual_content = False\n",
    "    for path in page.get_drawings():\n",
    "        if path['rect'].intersects(initial_roi) and path['rect'].width > 1 and path['rect'].height > 1:\n",
    "            content_bbox.include_rect(path['rect']); has_explicit_visual_content = True\n",
    "    for img_info in page.get_images(full=True):\n",
    "        try:\n",
    "            img_bbox_cand = page.get_image_bbox(img_info, transform=False)\n",
    "            if img_bbox_cand.intersects(initial_roi): content_bbox.include_rect(img_bbox_cand); has_explicit_visual_content = True\n",
    "        except: continue\n",
    "    if has_explicit_visual_content and not content_bbox.is_empty and content_bbox.width > 5 and content_bbox.height > 5:\n",
    "        roi = content_bbox.intersect(initial_roi)\n",
    "        if roi.is_empty or roi.width < 5 or roi.height < 5: roi = fitz.Rect(initial_roi)\n",
    "    elif not is_likely_table and (initial_roi.width < MIN_VISUAL_WIDTH_PYMUPDF or initial_roi.height < MIN_VISUAL_HEIGHT_PYMUPDF): return None\n",
    "    else: roi = fitz.Rect(initial_roi)\n",
    "    text_blocks = [fitz.Rect(b[:4]) for b in page.get_text(\"blocks\", flags=0) if b[6] == 0]\n",
    "    current_top_texts = sorted([tb for tb in text_blocks if tb.y1 < roi.y0 + 15 and (max(tb.x0, roi.x0) < min(tb.x1, roi.x1))], key=lambda x: x.y1, reverse=True)\n",
    "    for tb in current_top_texts:\n",
    "        if tb.get_area() > TEXT_BLOCK_MIN_AREA_FOR_OBSTRUCTION: roi.y0 = tb.y1 + 2; break\n",
    "    roi.intersect(page_rect)\n",
    "    if roi.is_empty or roi.width < MIN_VISUAL_WIDTH_PYMUPDF or roi.height < MIN_VISUAL_HEIGHT_PYMUPDF: return None\n",
    "    current_bottom_texts = sorted([tb for tb in text_blocks if tb.y0 > roi.y1 - 15 and (max(tb.x0, roi.x0) < min(tb.x1, roi.x1))], key=lambda x: x.y0)\n",
    "    for tb in current_bottom_texts:\n",
    "        if tb.get_area() > TEXT_BLOCK_MIN_AREA_FOR_OBSTRUCTION: roi.y1 = tb.y0 - 2; break\n",
    "    roi.intersect(page_rect)\n",
    "    return roi if roi.width >= MIN_VISUAL_WIDTH_PYMUPDF and roi.height >= MIN_VISUAL_HEIGHT_PYMUPDF else None\n",
    "\n",
    "print(\"PyMuPDF element extraction helpers defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3: Main Semantic Visual Element Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main semantic visual extractor function defined.\n"
     ]
    }
   ],
   "source": [
    "def extract_visual_elements_pymupdf(\n",
    "    doc_fitz: fitz.Document,\n",
    "    page_num: int, # 0-indexed\n",
    "    base_image_save_dir_for_pdf: str, # Specific to the PDF being processed\n",
    "    processed_captions_for_this_pdf: Set[str] # To track across pages of THIS PDF\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Extracts and processes visual elements (figures, tables) from a PDF page using PyMuPDF.\n",
    "    It distinguishes between actual visual content and textual descriptions of tables/figures.\n",
    "    Uses `generate_detailed_image_description` for VLM descriptions.\n",
    "    \"\"\"\n",
    "    page = doc_fitz.load_page(page_num)\n",
    "    visual_docs: List[Document] = []\n",
    "    # Ensure base_image_save_dir_for_pdf is created if it doesn't exist\n",
    "    os.makedirs(base_image_save_dir_for_pdf, exist_ok=True)\n",
    "\n",
    "    primitive_visuals = [] # List of {\"bbox\": Rect, \"type\": \"raster\"|\"drawing_cluster\", \"id\": str, \"raw_info\"?}\n",
    "    # 1. Extract raster images\n",
    "    for img_idx, img_info_fitz in enumerate(page.get_images(full=True)):\n",
    "        try:\n",
    "            bbox = page.get_image_bbox(img_info_fitz, transform=False)\n",
    "            img_bytes_check = doc_fitz.extract_image(img_info_fitz[0])[\"image\"]\n",
    "            if not img_bytes_check: continue\n",
    "            pil_img_check = Image.open(io.BytesIO(img_bytes_check))\n",
    "            if pil_img_check.width > MIN_VISUAL_WIDTH_PYMUPDF/2 and pil_img_check.height > MIN_VISUAL_HEIGHT_PYMUPDF/2 :\n",
    "                 primitive_visuals.append({\"bbox\": bbox, \"type\": \"raster\", \"id\": f\"p{page_num}_raster{img_idx}\", \"raw_info\": img_info_fitz})\n",
    "        except Exception: continue\n",
    "\n",
    "    # 2. Extract drawing clusters\n",
    "    for dc_idx, dc_bbox in enumerate(get_pymupdf_drawing_clusters(page)): # Uses helper\n",
    "        is_within_raster = any(r_info[\"type\"] == \"raster\" and r_info[\"bbox\"].contains(dc_bbox) for r_info in primitive_visuals)\n",
    "        if not is_within_raster:\n",
    "            primitive_visuals.append({\"bbox\": dc_bbox, \"type\": \"drawing_cluster\", \"id\": f\"p{page_num}_draw{dc_idx}\"})\n",
    "\n",
    "    captions_on_page = find_pymupdf_captions(page) # Uses helper\n",
    "    caption_to_visual_parts_map = defaultdict(list) # semantic_id -> [visual_part_data, caption_meta_data]\n",
    "    unassigned_primitives = list(primitive_visuals) # Copy\n",
    "    processed_primitives_indices_this_page = set()\n",
    "\n",
    "    # 3. Associate captions with primitives\n",
    "    for cap_info in captions_on_page:\n",
    "        if cap_info[\"id\"] in processed_captions_for_this_pdf:\n",
    "            # print(f\"Debug (Page {page_num+1}): Skipping already processed caption: {cap_info['id']}\")\n",
    "            continue\n",
    "        found_visual_for_caption = False; potential_visuals_for_caption = []\n",
    "        cap_info_bbox_center_y = (cap_info[\"bbox\"].y0 + cap_info[\"bbox\"].y1) / 2\n",
    "        for i, prim_vis in enumerate(unassigned_primitives):\n",
    "            if i in processed_primitives_indices_this_page: continue\n",
    "            prim_vis_bbox_center_y = (prim_vis[\"bbox\"].y0 + prim_vis[\"bbox\"].y1) / 2\n",
    "            v_dist = abs(prim_vis_bbox_center_y - cap_info_bbox_center_y)\n",
    "            h_overlap = max(0, min(prim_vis[\"bbox\"].x1, cap_info[\"bbox\"].x1) - max(prim_vis[\"bbox\"].x0, cap_info[\"bbox\"].x0))\n",
    "            h_overlap_ratio = 0\n",
    "            if prim_vis[\"bbox\"].width > 0 and cap_info[\"bbox\"].width > 0 :\n",
    "                 h_overlap_ratio_prim = h_overlap / prim_vis[\"bbox\"].width\n",
    "                 h_overlap_ratio_cap = h_overlap / cap_info[\"bbox\"].width\n",
    "                 h_overlap_ratio = max(h_overlap_ratio_prim, h_overlap_ratio_cap)\n",
    "            is_correctly_positioned = False # Logic from your notebook\n",
    "            max_v_dist_figure = 200; max_v_dist_table = 100\n",
    "            if cap_info[\"type\"] == \"figure\":\n",
    "                if ((prim_vis_bbox_center_y < cap_info_bbox_center_y and v_dist < max_v_dist_figure) or (abs(v_dist) < 40)) : is_correctly_positioned = True\n",
    "            elif cap_info[\"type\"] == \"table\":\n",
    "                if ((prim_vis_bbox_center_y > cap_info_bbox_center_y and v_dist < max_v_dist_table) or (abs(v_dist) < 40)) : is_correctly_positioned = True\n",
    "            if h_overlap_ratio > 0.2 and is_correctly_positioned: potential_visuals_for_caption.append((prim_vis, i))\n",
    "\n",
    "        if potential_visuals_for_caption: # Associate best match\n",
    "            potential_visuals_for_caption.sort(key=lambda x: (0 if cap_info[\"type\"] == \"figure\" and x[0][\"type\"] == \"drawing_cluster\" else 1, abs((x[0][\"bbox\"].y0 + x[0][\"bbox\"].y1)/2 - cap_info_bbox_center_y), -(x[0][\"bbox\"].width * x[0][\"bbox\"].height)))\n",
    "            best_prim_vis, best_prim_idx = potential_visuals_for_caption[0]\n",
    "            target_list = caption_to_visual_parts_map[cap_info[\"id\"]]\n",
    "            if not any(item.get(\"is_caption_meta\") for item in target_list): target_list.append({\"is_caption_meta\": True, \"caption_obj\": cap_info})\n",
    "            target_list.append(best_prim_vis)\n",
    "            processed_primitives_indices_this_page.add(best_prim_idx); found_visual_for_caption = True\n",
    "            # print(f\"Debug (Page {page_num+1}): Matched caption {cap_info['id']} with visual {best_prim_vis['id']}\")\n",
    "\n",
    "        if not found_visual_for_caption and cap_info[\"id\"] not in processed_captions_for_this_pdf: # Handle textual captions\n",
    "            doc_type = \"text_table_content\" if cap_info[\"type\"] == \"table\" else \"text_figure_description\"\n",
    "            content_to_add = cap_info[\"text\"]\n",
    "            if cap_info[\"type\"] == \"table\":\n",
    "                table_body_content = find_table_content(page, cap_info[\"bbox\"]) # Uses helper\n",
    "                if table_body_content: content_to_add = f\"{cap_info['text']}\\n\\n{table_body_content}\"\n",
    "            visual_docs.append(Document(page_content=content_to_add, metadata={\"source\": doc_fitz.name, \"page_number\": page.number + 1, \"type\": doc_type, \"original_caption\": cap_info[\"text\"], \"caption_id\": cap_info[\"id\"], \"element_subtype\": \"textual_content_with_caption\"}))\n",
    "            processed_captions_for_this_pdf.add(cap_info[\"id\"])\n",
    "            # print(f\"Debug (Page {page_num+1}): Added textual content for {doc_type}: {cap_info['id']}\")\n",
    "\n",
    "    # 4. Handle unassigned large primitives\n",
    "    for i, prim_vis in enumerate(unassigned_primitives):\n",
    "        if i in processed_primitives_indices_this_page: continue\n",
    "        if prim_vis[\"bbox\"].width > MIN_VISUAL_WIDTH_PYMUPDF * 2 and prim_vis[\"bbox\"].height > MIN_VISUAL_HEIGHT_PYMUPDF * 2:\n",
    "            caption_to_visual_parts_map[prim_vis[\"id\"]].append(prim_vis) # Use primitive's own ID\n",
    "            # print(f\"Debug (Page {page_num+1}): Found uncaptioned large visual: {prim_vis['id']}\")\n",
    "\n",
    "    # 5. Process semantic visual groups (render, get VLM desc)\n",
    "    for semantic_id, group_elements in caption_to_visual_parts_map.items():\n",
    "        if semantic_id in processed_captions_for_this_pdf and any(item.get(\"is_caption_meta\") for item in group_elements): pass # Allow re-processing if visual part exists for an already text-processed caption\n",
    "        elif semantic_id in processed_captions_for_this_pdf: continue # Fully processed\n",
    "\n",
    "        visual_parts_data_list = [item for item in group_elements if not item.get(\"is_caption_meta\")]\n",
    "        caption_meta_item = next((item for item in group_elements if item.get(\"is_caption_meta\")), None)\n",
    "        if not visual_parts_data_list: continue # No visual part to render\n",
    "\n",
    "        composite_render_roi = fitz.Rect()\n",
    "        final_caption_text = f\"Uncaptioned Visual ({semantic_id})\"\n",
    "        final_semantic_type = \"figure\"\n",
    "        if caption_meta_item:\n",
    "            current_caption_info = caption_meta_item[\"caption_obj\"]\n",
    "            final_caption_text = current_caption_info[\"text\"]; final_semantic_type = current_caption_info[\"type\"]\n",
    "            composite_render_roi.include_rect(current_caption_info[\"bbox\"])\n",
    "        for part_data in visual_parts_data_list: composite_render_roi.include_rect(part_data[\"bbox\"])\n",
    "        composite_render_roi.intersect(page.rect)\n",
    "        if composite_render_roi.is_empty or composite_render_roi.width < MIN_VISUAL_WIDTH_PYMUPDF or composite_render_roi.height < MIN_VISUAL_HEIGHT_PYMUPDF: continue\n",
    "\n",
    "        is_table_visual = final_semantic_type == \"table\"\n",
    "        refined_roi = refine_roi_by_content_and_text(page, composite_render_roi, is_table_visual) # Uses helper\n",
    "        if not refined_roi: continue\n",
    "\n",
    "        try:\n",
    "            pix = page.get_pixmap(clip=refined_roi, dpi=RENDER_DPI_PYMUPDF, alpha=False)\n",
    "            filename_part = re.sub(r'[^\\w.-]', '_', str(semantic_id))\n",
    "            base_filename = f\"page{page_num+1}_SEMANTIC_{filename_part}.png\"\n",
    "            image_save_path = os.path.join(base_image_save_dir_for_pdf, base_filename)\n",
    "            counter = 0 # Ensure unique filename\n",
    "            while os.path.exists(image_save_path):\n",
    "                counter += 1\n",
    "                image_save_path = os.path.join(base_image_save_dir_for_pdf, f\"page{page_num+1}_SEMANTIC_{filename_part}_{counter}.png\")\n",
    "            pix.save(image_save_path)\n",
    "\n",
    "            vlm_description = generate_detailed_image_description(image_save_path, f\"visual element ({final_semantic_type})\") # Uses helper\n",
    "\n",
    "            visual_docs.append(Document(page_content=vlm_description, metadata={\"source\": doc_fitz.name, \"page_number\": page_num + 1, \"type\": \"image_description\", \"image_path\": image_save_path, \"original_caption\": final_caption_text, \"element_subtype\": f\"pymupdf_semantic_{final_semantic_type}\", \"caption_id\": semantic_id}))\n",
    "            processed_captions_for_this_pdf.add(semantic_id) # Mark as fully processed (rendered + described)\n",
    "            # print(f\"Debug (Page {page_num+1}): Rendered and described visual: {semantic_id} to {os.path.basename(image_save_path)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error rendering/describing semantic visual '{semantic_id}' on page {page_num+1}: {e}\")\n",
    "    return visual_docs\n",
    "\n",
    "print(\"Main semantic visual extractor function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Baseline: Pure Textual RAG\n",
    "\n",
    "Before diving into multimodal complexities, let's establish a baseline using a standard text-only RAG pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Text Extraction and Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 196 text chunks from 2 PDFs.\n",
      "\n",
      "Sample Text Chunk:\n",
      "Source: AAG.pdf, Page: 0\n",
      "Pairing Analogy-Augmented Generation with Procedural Memory for\n",
      "Procedural Q&A\n",
      "K Roth1*, Rushil Gupta1*, Simon Halle2, Bang Liu1,3†\n",
      "1Université de Montréal & Mila, 2Thales Canada, 3Canada CIFAR AI Chair\n",
      "{kyle.roth, rushil.gupta, bang.liu}@umontreal.ca\n",
      "simon.halle@thalesgroup.com\n",
      "Abstract\n",
      "Large language models struggle to synthesize\n",
      "disparate pieces of information into a coherent\n",
      "plan when approaching a complex procedural\n",
      "task. In this work, we introduce a novel formal-\n",
      "ism and structure for such...\n"
     ]
    }
   ],
   "source": [
    "# Code for basic text extraction using PyMuPDF (or Unstructured)\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "documents = []\n",
    "for pdf_path in PDF_PATHS:\n",
    "    loader = PyMuPDFLoader(pdf_path)\n",
    "    docs_for_pdf = loader.load()\n",
    "    # Add source and page number metadata more explicitly if PyMuPDFLoader doesn't do it well enough\n",
    "    for doc in docs_for_pdf:\n",
    "        doc.metadata[\"source\"] = os.path.basename(pdf_path)\n",
    "    documents.extend(docs_for_pdf)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "text_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Extracted {len(text_chunks)} text chunks from {len(PDF_PATHS)} PDFs.\")\n",
    "if text_chunks:\n",
    "    print(\"\\nSample Text Chunk:\")\n",
    "    print(f\"Source: {text_chunks[0].metadata.get('source', 'N/A')}, Page: {text_chunks[0].metadata.get('page', 'N/A')}\")\n",
    "    print(text_chunks[0].page_content[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Building Textual Vector Store and RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target ChromaDB baseline directory: /home/dipesh/WORKSPACE/LET's Learn/vtierp_project/notebooks/exploration_outputs_v2/chroma_stores_notebook/text_baseline_store\n",
      "Created parent directory for Chroma stores: /home/dipesh/WORKSPACE/LET's Learn/vtierp_project/notebooks/exploration_outputs_v2/chroma_stores_notebook\n",
      "Ensured specific ChromaDB directory exists: /home/dipesh/WORKSPACE/LET's Learn/vtierp_project/notebooks/exploration_outputs_v2/chroma_stores_notebook/text_baseline_store\n",
      "\n",
      "Attempting to create ChromaDB with 196 documents...\n",
      "ChromaDB for baseline created successfully with 196 entries.\n",
      "\n",
      "--- Baseline Textual RAG Query Test ---\n",
      "\n",
      "Q: What is AAG?\n",
      "A: AAG is a system that takes a user query as input and outputs a bulleted list of steps that will lead the user to the desired goal. It uses a custom procedure memory store to retrieve and adapt specialized domain knowledge to answer new procedural tasks.\n",
      "\n",
      "Q: What does Figure 1 in AAG.pdf show?\n",
      "A: The provided text does not contain any information about what Figure 1 in AAG.pdf shows. It only refers to Figure 2.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import os\n",
    "import shutil\n",
    "import chromadb # Import chromadb directly for client settings if needed\n",
    "\n",
    "# Ensure these are defined and populated from previous cells:\n",
    "# NOTEBOOK_DIR (e.g., os.getcwd())\n",
    "# text_chunks (list of Langchain Document objects)\n",
    "# embeddings (initialized GoogleGenerativeAIEmbeddings instance)\n",
    "# llm_rag (initialized ChatGoogleGenerativeAI instance)\n",
    "\n",
    "# --- Vector Store Creation for Baseline ---\n",
    "# Define a unique and absolute path for this baseline text vector store\n",
    "# Using a subdirectory structure for clarity\n",
    "baseline_chroma_parent_dir = os.path.join(NOTEBOOK_DIR, \"exploration_outputs_v2\", \"chroma_stores_notebook\") # Parent for all notebook chromas\n",
    "text_vectorstore_path_baseline = os.path.join(baseline_chroma_parent_dir, \"text_baseline_store\")\n",
    "\n",
    "print(f\"Target ChromaDB baseline directory: {text_vectorstore_path_baseline}\")\n",
    "\n",
    "# 1. Ensure the parent directory exists\n",
    "if not os.path.exists(baseline_chroma_parent_dir):\n",
    "    try:\n",
    "        os.makedirs(baseline_chroma_parent_dir, exist_ok=True)\n",
    "        print(f\"Created parent directory for Chroma stores: {baseline_chroma_parent_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating parent directory {baseline_chroma_parent_dir}: {e}\")\n",
    "        raise\n",
    "\n",
    "# 2. Forcefully remove the specific ChromaDB directory if it exists to ensure a clean start\n",
    "if os.path.exists(text_vectorstore_path_baseline):\n",
    "    print(f\"Removing existing ChromaDB directory: {text_vectorstore_path_baseline}\")\n",
    "    try:\n",
    "        shutil.rmtree(text_vectorstore_path_baseline)\n",
    "    except Exception as e:\n",
    "        print(f\"Error removing directory {text_vectorstore_path_baseline}: {e}. Please check permissions or close other processes using it.\")\n",
    "        raise\n",
    "\n",
    "# 3. Explicitly create the target ChromaDB directory (Chroma usually does this, but this is a belt-and-suspenders approach)\n",
    "try:\n",
    "    os.makedirs(text_vectorstore_path_baseline, exist_ok=True)\n",
    "    print(f\"Ensured specific ChromaDB directory exists: {text_vectorstore_path_baseline}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating specific ChromaDB directory {text_vectorstore_path_baseline}: {e}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "# --- Attempt to initialize ChromaDB ---\n",
    "text_vectorstore_baseline = None\n",
    "text_retriever_baseline = None\n",
    "\n",
    "if 'text_chunks' in locals() and text_chunks and 'embeddings' in locals() and embeddings:\n",
    "    print(f\"\\nAttempting to create ChromaDB with {len(text_chunks)} documents...\")\n",
    "    try:\n",
    "        # Simplest form first\n",
    "        text_vectorstore_baseline = Chroma.from_documents(\n",
    "            documents=text_chunks,\n",
    "            embedding=embeddings,\n",
    "            persist_directory=text_vectorstore_path_baseline\n",
    "            # collection_name=\"baseline_text_collection\" # Optional: good practice\n",
    "        )\n",
    "        # text_vectorstore_baseline.persist() # Often redundant if persist_directory is used in from_documents\n",
    "\n",
    "        # Verify creation\n",
    "        if text_vectorstore_baseline and text_vectorstore_baseline._collection:\n",
    "             print(f\"ChromaDB for baseline created successfully with {text_vectorstore_baseline._collection.count()} entries.\")\n",
    "             text_retriever_baseline = text_vectorstore_baseline.as_retriever(search_kwargs={\"k\": 5})\n",
    "        else:\n",
    "            print(\"ChromaDB object or collection is None after creation attempt.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"!!!!!!!!!! ERROR CREATING CHROMADB VECTOR STORE !!!!!!!!!!!\")\n",
    "        print(f\"Path attempted: {text_vectorstore_path_baseline}\")\n",
    "        print(f\"Error details: {e}\")\n",
    "        print(\"\\nCommon troubleshooting steps:\")\n",
    "        print(\"1. Restart the Jupyter Kernel (Kernel -> Restart Kernel).\")\n",
    "        print(f\"2. Manually check permissions for the directory: '{baseline_chroma_parent_dir}' and its parent.\")\n",
    "        print(f\"   In terminal: `ls -ld \\\"{os.path.abspath(baseline_chroma_parent_dir)}\\\"`\")\n",
    "        print(f\"   Ensure your user (who runs Jupyter) can write to it.\")\n",
    "        print(f\"3. Try an even simpler path, e.g., in your HOME directory, to rule out complex path issues: `os.path.join(os.path.expanduser('~'), 'chroma_test_db')`\")\n",
    "        print(\"4. Ensure no other process is locking files in this directory.\")\n",
    "        print(\"5. If using WSL or a virtualized environment, ensure filesystem interactions are smooth.\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        text_vectorstore_baseline = None # Ensure it's None on failure\n",
    "else:\n",
    "    print(\"Skipping ChromaDB creation for baseline: `text_chunks` or `embeddings` model not available or empty.\")\n",
    "\n",
    "\n",
    "# --- RAG Prompt and Chain (only if ChromaDB and LLM are ready) ---\n",
    "if text_retriever_baseline and 'llm_rag' in locals() and llm_rag:\n",
    "    template_baseline = \"\"\"Answer the question based only on the following context:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Answer:\"\"\"\n",
    "    prompt_baseline = ChatPromptTemplate.from_template(template_baseline)\n",
    "\n",
    "    def format_docs_simple(docs): # Make sure this helper is defined\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    rag_chain_baseline = (\n",
    "        {\"context\": text_retriever_baseline | format_docs_simple, \"question\": RunnablePassthrough()}\n",
    "        | prompt_baseline\n",
    "        | llm_rag\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # --- Test Baseline RAG ---\n",
    "    print(f\"\\n--- Baseline Textual RAG Query Test ---\")\n",
    "    baseline_query_1 = \"What is AAG?\"\n",
    "    try:\n",
    "        baseline_answer_1 = rag_chain_baseline.invoke(baseline_query_1)\n",
    "        print(f\"\\nQ: {baseline_query_1}\")\n",
    "        print(f\"A: {baseline_answer_1}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error invoking RAG chain for query '{baseline_query_1}': {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "    baseline_query_2 = \"What does Figure 1 in AAG.pdf show?\"\n",
    "    try:\n",
    "        baseline_answer_2 = rag_chain_baseline.invoke(baseline_query_2)\n",
    "        print(f\"\\nQ: {baseline_query_2}\")\n",
    "        print(f\"A: {baseline_answer_2}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error invoking RAG chain for query '{baseline_query_2}': {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    if not text_retriever_baseline:\n",
    "        print(\"Skipping RAG chain test: `text_retriever_baseline` not available (ChromaDB creation likely failed).\")\n",
    "    if 'llm_rag' not in locals() or not llm_rag:\n",
    "        print(\"Skipping RAG chain test: `llm_rag` not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Observations and Limitations\n",
    "\n",
    "The text-only RAG performs reasonably well for purely textual queries. However, when asked about visual elements like figures (e.g., \"What does Figure 1 show?\"), its ability is limited to any textual mentions of that figure. It cannot \"see\" or understand the visual content itself. This highlights the need for incorporating visual information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Towards Multimodal RAG: Incorporating Visual Elements\n",
    "\n",
    "To address the limitations, we need to extract and represent visual information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Identifying and Extracting Visual Primitives (Images & Drawings)\n",
    "\n",
    "We'll start by using PyMuPDF to identify and extract raster images and vector drawings (drawing clusters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Visual Primitive Exploration on AAG.pdf, Page 1 ---\n",
      "Found 3 raster image objects.\n",
      "  Raster 0: XREF 198, BBox Rect(442.07269287109375, 236.32191467285156, 516.5596923828125, 333.72796630859375)\n",
      "  Raster 1: XREF 199, BBox Rect(358.12261962890625, 264.4487609863281, 403.32916259765625, 312.1667785644531)\n",
      "  Raster 2: XREF 200, BBox Rect(437.9915466308594, 333.72796630859375, 464.3216857910156, 365.4670104980469)\n",
      "Found 1 drawing clusters (potential diagrams/vector graphics).\n",
      "  Drawing Cluster 0: BBox Rect(302.80645751953125, 222.96961975097656, 504.3468933105469, 350.8435974121094)\n",
      "Found 1 potential captions.\n",
      "  Caption: 'Figure 1: An illustration of using the AAG system ...' (Type: figure, ID: Figure1)\n"
     ]
    }
   ],
   "source": [
    "# These cells now demonstrate the usage of functions defined in Cell 5.2\n",
    "\n",
    "# Example of applying it to a page:\n",
    "doc_fitz_example = fitz.open(PDF_PATHS[0])\n",
    "example_page_num = 0 # First page of AAG.pdf\n",
    "example_page = doc_fitz_example.load_page(example_page_num)\n",
    "\n",
    "print(f\"\\n--- Visual Primitive Exploration on {os.path.basename(PDF_PATHS[0])}, Page {example_page_num+1} ---\")\n",
    "raster_images_info = example_page.get_images(full=True)\n",
    "print(f\"Found {len(raster_images_info)} raster image objects.\")\n",
    "for i, img_info in enumerate(raster_images_info):\n",
    "    try:\n",
    "        bbox = example_page.get_image_bbox(img_info)\n",
    "        print(f\"  Raster {i}: XREF {img_info[0]}, BBox {bbox}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error getting bbox for raster {i}: {e}\")\n",
    "\n",
    "\n",
    "drawing_clusters = get_pymupdf_drawing_clusters(example_page)\n",
    "print(f\"Found {len(drawing_clusters)} drawing clusters (potential diagrams/vector graphics).\")\n",
    "for i, bbox in enumerate(drawing_clusters):\n",
    "    print(f\"  Drawing Cluster {i}: BBox {bbox}\")\n",
    "\n",
    "captions_found = find_pymupdf_captions(example_page)\n",
    "print(f\"Found {len(captions_found)} potential captions.\")\n",
    "for cap in captions_found:\n",
    "    print(f\"  Caption: '{cap['text'][:50]}...' (Type: {cap['type']}, ID: {cap['id']})\")\n",
    "\n",
    "doc_fitz_example.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Visual Primitive Exploration on AAG.pdf, Page 1 ---\n",
      "Found 3 raster image objects.\n",
      "Found 1 drawing clusters.\n",
      "Found 1 potential captions.\n"
     ]
    }
   ],
   "source": [
    "# Another Example of applying them to a page:\n",
    "if PDF_PATHS and os.path.exists(PDF_PATHS[0]):\n",
    "    doc_fitz_example = fitz.open(PDF_PATHS[0])\n",
    "    example_page_num = 0 # First page of AAG.pdf (or a page you know has interesting elements)\n",
    "    if len(doc_fitz_example) > example_page_num:\n",
    "        example_page = doc_fitz_example.load_page(example_page_num)\n",
    "\n",
    "        print(f\"\\n--- Visual Primitive Exploration on {os.path.basename(PDF_PATHS[0])}, Page {example_page_num+1} ---\")\n",
    "        raster_images_info = example_page.get_images(full=True)\n",
    "        print(f\"Found {len(raster_images_info)} raster image objects.\")\n",
    "        # ... (rest of your demo code from old Cell 10) ...\n",
    "\n",
    "        drawing_clusters = get_pymupdf_drawing_clusters(example_page) # Uses function from Cell 4.2\n",
    "        print(f\"Found {len(drawing_clusters)} drawing clusters.\")\n",
    "        # ...\n",
    "\n",
    "        captions_found = find_pymupdf_captions(example_page) # Uses function from Cell 4.2\n",
    "        print(f\"Found {len(captions_found)} potential captions.\")\n",
    "        # ...\n",
    "        doc_fitz_example.close()\n",
    "    else:\n",
    "        print(f\"PDF {PDF_PATHS[0]} has less than {example_page_num+1} pages.\")\n",
    "else:\n",
    "    print(f\"PDF path {PDF_PATHS[0] if PDF_PATHS else 'N/A'} not found or PDF_PATHS is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Semantic Grouping and Generating Descriptions for Visuals\n",
    "\n",
    "The raw primitives need to be grouped into meaningful semantic units (e.g., a diagram with its caption). We also need to generate descriptions for these units, especially for uncaptioned visuals or to augment existing captions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.1. Associating Captions with Visuals and Handling Textual Tables\n",
    "This is where the core logic of  `extract_visual_elements_pymupdf`comes in. It attempts to:\n",
    "*   Match captions to nearby visual primitives.\n",
    "*   Identify and extract content from textual tables based on their captions.\n",
    "*   Handle uncaptioned but significant visual elements.\n",
    "*   Render these semantic visual groups as images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Semantic Visual Element Extraction and Description Demo ---\n",
      "\n",
      "Generated 2 document objects from semantic visual extraction on page 1:\n",
      "\n",
      "Document 1 (Type: image_description, Subtype: pymupdf_semantic_figure, CaptionID: Figure1)\n",
      "  Content (Description/Caption): This visual element is a **flowchart or process diagram** illustrating how a Large Language Model (LLM) might leverage \"procedural memory\" to solve a new problem by drawing on analogical examples. **Key Visual Components & Structure:** 1. **Central Figure (LLM):** A stick figure with \"LLM\" written o...\n",
      "  Saved Image: page1_SEMANTIC_Figure1.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {
      "image/png": {
       "width": 200
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 2 (Type: image_description, Subtype: pymupdf_semantic_figure, CaptionID: p0_raster0)\n",
      "  Content (Description/Caption): This visual element is a **conceptual diagram or illustration**, presented in a simple, hand-drawn style. **Key Visual Components:** * **Main Object:** A rectangular piece of paper or card, oriented vertically. * **Holding Element:** A hand, drawn in a minimalist, cartoon-like style (showing a thumb...\n",
      "  Saved Image: page1_SEMANTIC_p0_raster0.png\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJ0AAAC8CAIAAADttDsSAAAACXBIWXMAABcSAAAXEgFnn9JSAAA300lEQVR4nO2dB1wU1/r3k9yb5N7kn568yS2yld57EwQUQZq9gKL0pogKKhZQQbFjQ7HFEhuiWFBRsKECKggoKthBRBEEG4qgxn1/Mwcm61IUtsjifZjPfmaGM+1852lnzpz5RNAx5MCBA0aGhhpq6uqqapjUVFRVlVXIxMxjBgW0NDS11TV1tLR1tXUMdPQM9fWNDQ2NjYzNjIyNjYyMDY1MjEwwb0JPpsYm+DUyMMSEf1ElDekZlNQ3MGyc9HX1DHX1DPT0sTcDfX09HV3sXF9bR5ee9N+adJl53YZJV09Hh54attLT1cM8Jh1taqW+LiY9kUmP/sVZGQqdBg7dr1+/IUOGjBs3rr6uTpz6/ERSYMSU1atXczlcDTU1TQ1NQpfBqd44aYKrioqWqhpmtNTUtdU1MOloaOqCNFZSkyr+pY5fVTUNFVU1JSV1JWU1RSVVRSUVPl+Fr6jC42NS5vGUuTwloYla5HCVyC+Hq4iJzVHkcMivEjVxmYla2TjxyS+LTeb5bMyQeTYWeSxMbJ4Ci/5tmEFhrgKL24WFX3oNq6EAi81hsXHhKkrKgwcOqnn2TJz67BBc37x5s2zZMkW+4qr4lXl5ednZ2aezsk6dPHkiPT39OKbjmE6kn8g4lZGZkXnm9JmzZ87k5OTknsvFlEdP2WfPkuns6TNZGZkohs2PHT129MiRtNTUlAMp+5KT9+7ek5SUlJiwfduWrZv+2LRx/YZ1v6/7fe3a1StXrYyPj1sWF7ds2dLFS2IXxsYuXLhg3oKYWbNiZs6aFT0zOipqZlQ0fqNmRE2LiIyYMjViyhT8Tp08ZXJ4+CQyTQyfEDY+bFxoWGjYePxA48aOGzdm7LixY8eNGTN6VHDwyFHkd2RgUGBAYFBAQFBAoL+vr7eXFzV5enl5evp6+8SvWNHDxqZfn75PnjwRp0o7Ctf58+cr8vm7du360OdCnUz75M/3ltdN5OXLl89pqa6u7mHT3cXZ+dGjx+JcRYfgikuNiYnh83jbt29/8+ZDn80HlaoHD6wsu7k4OT96+Eic/XQYrrNieFze1q1b39BgX756eeNuEdbfvHdle/rauvoXH/ocKcFpVD99INVDlJeXdzUzd3JwfFhdLc5+OgzXmBguj7t582bC9WD2Tu7wT3aeXL9q/1zMnLua0b49v3r9SoLnOTthvJLn33HPSXCfIlJ2546ZiWkvO/uqqipx9tMhuILlvHnzeDzetm3byJrUc7uA805l8aoD81S8v2zfbksrb6l4fbHjxDpJnWfU5hCj4F8ltbdmpaS42MTAyMG+V3WV/OsruCIIBdfExESir4dyksD18bOHy/ZGq/n8kxSD1m49tvJe9R3MXyu7fLkkHzMPHt8/dy2TFFifujglewezW5TETqZuCEhIXyOyubDkXD2VfuEgma+pfbr12Kqj+fuYEztwNnHnqQ0v6msFNFeT0f/CTP2r+t2Zm57XiZWKNCu3bt1CUgv/+vDhQ3H201G4Ll26lM/n79y5U4Trwh1TYPqw8sLNbKxxnKqtH/QTnJxPrJP9ZA2UnJMwAeuLSi/A5KLk0t0zyD4LbuWMXj4E/7KbpDZhjafI5syhTxakKnt+ruH7NSw/Fr0WOpiO/jfKLEqKxOLqlPmq3v/oPkFpyCxLQSNXeI1xK4dhb8fPH5B4Vdy8cdNQ34CKmx7Jf9wEbPEr4pWUlPbs2UPWACcqTtPv//CLCaHTlPX+vSZroqRe4I9Y3HI0Hjxq6573n2GCAnuztpy6mKbo8beHNQ1uad+ZhJHLBuBfZ4rSsSiyOXNoz/n2Q+fYDJ5pMSpuYOWjcpSHC9iQusQs5D/4b4+JytM3BeOmwfqS+zfA1WDkLwQqbIM0quLG9ev6evq9HZ3E5VpYWCipcxJHVq9eraKicuBAgwbAGKLuYraGTlzrBVqweNCYsFXD8S/rMN6S3dOL719HgcQTv+NXO+C7yev8wtd6D59nK7zPW+VXGa4imzNloJoRGwIBEpxOFx4nTn1P5mbMvP7zNW/Ep4jdyh+Wkf2AK7nPViTHSKkewNVAT7+PS+/Hj8XLX0tLSw8fPizmXsSXNWvWgOvBgw1+DiYO1Qflg0eEXmKNa0w3wIPCIYwiumI+pgu0B1Ch3KAFRdx2fLXwPq/fLWS4Nt2cCAKruL0zr965BD+KkigPrQU27Bb2lj/is32nt+Vez8L6wtvnCVfAhn1+I51E+/q1a/q6epJpb3r9+vWZM2fOnz8vpXN9HxHhCreHGqx6UjkvMRwzqOLpf4yCtbxSWoDFvOunUWb8Gg/Mw9gibiJqJJJcghZWZhUew3zTzYnAdyI0gyceNLNr1ZMKgET5gCV9R8zrif/ChccmRWxIW6rl/23dyzpwBW/PBb2wk2FzuheXX5N4PVwpKtLV1unXV3LtiBUVFampqbW1tWKfW3uEcE1JSSGLxy+koO5gSGFpnabqANjNe1e0/L6BxyU1DoGbRJmNh5fBy8JW478i+4RHRIG52ydivunmRDYfWYFtdQN/gGHHInQaMRSUmGg53DZiMYROxPBGbgzCf3H3w7ujDO4ViddDkcS5Ql6+fAmbXFZWJt65tUfWrl2rKsQV+caq/XMf1byVwyGlQU7CNDU8e1GTfHorVBnzUFkm2xEWpChX71xsdnNGoMT4F5kHs/wbZ8oelDD/xQ1BEipIxaN7sMZkHmUk2+hBhOaqLWGuRHJyci5fvizOTtshRF/3798n4+N2NAFXHSlxhVymRZz9tkmoPCc+XllZOXlvMuPjEY5CUUiDwMcjNFfY4X7Sek6H/OfKlSvi7Pr9hXBF/rpr9643ggauwXGD4B2twrhPnr07k1u6ewZpSXh/Wbhz6u8HY9tzutKUosIiHS2p6SuR3Nzc27dvi7P39xRwXbFiBfSVam+iucIpAioySxIZvXMPzhG6yHTbdFCHKVqj4ga284ylJtAl6GtfaT9XT09PF/MA7yPgGhcXp8hXZNqH1x5cSPIWk9H/8lvkImjSVHunshhZEDKNWVvH3a26jXz3dOFxsrei0guHcpJIzoMyR/Io2w6rjvCYFMDKrMtHg5b2D1s9QkA39iLuPZa/H/PYObIjzGxIXSLBBwbvL+CKeLhv7z7S5frq1au0tDRxDvA+QvrB8Pn8hIQEsiZkhSuyC8xApTzn2wuaNNWahfzHd5Gz+ZgurjHdbtwtshjHGhht/vLVS//FfdR9vkJKgywTxXZnbiJNCh7z7UjmCoo24xVJKxXhCsA6Ad8jbzl3NYPkTkhgkM/giNK+8KZSVFgIOyx1fYUg7SkoKBDnGO8UimtcHLhu27aVrEGWCc8qXEakqdZxqjZm3GZbk6ehQDV7WxgU0Xo8Hwo3YY2nwchfBHQrMYqRh7jEpKee242Z9AsH8QuupK0RZr/vdKOkUxvTcvdgERnt8fMpMAb7Tm9DzkpSW9kIFTfJhivk2LFjUm2Kouzwsjgej7dlS8NzdQCznaiCmRMFh6I2h7yofyHSVAuuKPD0eUPzp3UYj7Q/APyAKFMt/29JAy/hqh/007I9UaRdd8nu6fwRnyHrhcaDKywwTH3quV1QX9wWhOvWY6swD7o9w1XHxg8tuHVOetcuIjLlWlFRcenSJXEO07qA5ZIlS3hc3qY//iBcYX6Ng3/DzPB5trC99S/rRZpqwZWAJAI7jMWyByWwn9BO8sil8lE54QrdfVhThZm4vTNjkyJgqLEJcIIrDC/4GYz8+eyVE1hJuNa9pPru1tQ+ld4ltyQy5Qo5deqUOIdpXSiuixZx2Jz169cTrhvSlqJ+D+fuhZrCRwqaNNUimkXExOyhWygnZmsooir4YIRRXccqkJZhwhUaD0iYQW6DvWGfyIyJHYZPxcy96jvzEsNnbhlLrPSzFzXSu9jWRdZcc3JyXr6UVr8eiuvixTwOd/36DYQrDKzlODblQWOsyBqRptpekzXd5/Zg9gDv6BPrdLE4F5tAHaHZ8McoSfRv/5ntdfUvXCL116QseFr7BDcB9oOYC14cO0fAhTgLa04WpJL+N802ScpGCgsLtbW0+/TuLSOu5eXlN27cEOdIrQgqd1FsLJfDWbtmLePIoTTIRoTbm4SbahHvQC+Zf8GtknZgsLlUkiegsxf81tY9B0uRdmb863ndM2RK8NYCuu8jzDtx1ZjPvnLyAz7XunjxopaGZm9nFxlxhbJmZ2eLc6RWBPUYu2AB7PCa1as/YJ12BLlYUKClqSn1/FVYsrKyxDlSK0JzXchhsePj4//8uLleOH9BQ01dplylFzo1cGWz41fEk+duzHpYxaY9CGUsMM7HL6QQoy1tOX/+vLqqeh8XWflXiFTt8EJaX5ctW0a4HjibiPQUqQ6iGP/FfSR+xNYfnV4pLUCixdxPpL+q10IHiZ9GUzmfn6+hoiY7fX38+LH0ergRrlw2O27pUsJ185EVqMqQFa6nC4+LRD3iyzv7i5Ns5/rdv65XzeefI5cNkOxpNCt5uXnqyqqyy3MKCgoePJDWqymkvz+HxUIWS7j+cTgONct0IDp1MY30Yah6Uvn4WUOHaaY/Nza/VnYZkTMWkePCbG46shzwYDlJ6z/y3eTTW5lYWqS/OPihfOWjcvLfM0XpGZcOo4BwrwnT0f8eNqe78DkzW+FwW47GIxpHUI0dktyXsQfCndGbfQghInm5uWqq4CorfU1PTxfnMK0LWM6ZMwd2eHFsA1fSLsE0zFqFcQOX9su8dARZJumgJNyfm/RjmrzOF1o4ca0XiGJx35mE0FXuSFJRePTyIUbBv6I8UIn0F0fKi63wX4Jt56kN+JdOwPciXO0nayD9ZRaFt0LWhMR62d5o6zAeyZWHzrFR8f4St5RIZ/SmDyGaVkVOdo6qsooUn6sLS319/enTzZyEpKSRK2vRwtgGrqlLcOW+i5wn/e4DhegxURnYuoVysFIv8EfB2/25kXqSZv2FO6YI6J5HpPrGr/EYEG2GNajlRUmRpAFLpL948PLBuGOg6Jbj2AL6oSzWhK0aLsIVBxJ+DiGyFW6vgdHm5Bx2nlxPZrC5SGf0pg8hYHug5SuSY5Bkkz2fPXNWia8oo/eac3Jynj6VYmMpWM6ePRtc582Z+/r1a0Gjf4X1A9qntU/AFbe8zXjFsfFDSXdi4f7chCucMdnbpZI8LN6uuInCbrOtBfRT957hqsuTZ9XWPRe83V/cejx/3aFF/WeYzE+cBPPIH/HZ0t0zyNM6Ya5+i1zI/UREeCsBzRXlof349Yl1cpqqQzYX6Yze9CHE9E3Bmn7/h3t04c6pZM9nTp9R4vNl4V+hrCdPnhTnGO8Uoq+Im+bOnk24kpqFzhFvCq5YBMWZW8YqevxN8HZ/bsL1RMEhsjfSHRy/iGD9F/cW0BoMBrCWpHFfuL+4Wch/sB6WGR4RXLF+dcr8bcdXi3CF6gP53arb8KAoJrwV/gs2oA43ga1wYszmIp3Rmz6EgCkizxgYycrMUlJUlAXXEydO1Ik3Nsk7pZErZ05MDOFK/Bz0jDfi06RTG8EVOOGiwBVrUB3C/bmfPHuEwsxLcDW1T1F4b9YW8zH/nZ0w/ty1TBhD1DJ8MxRF8HZ/cTjI8LXeCH/giSse3YOxjdocQnqcC3OFOyA9/bn061ZNtxo+zxapNv7bd7oR7jCyuUhn9KYPIZpWRVZmpiKPL/W46datW1evXhXnAO8j5L1m2OFZ0dFEA0j7O/JXXDzqhTxtxXpEnv1mGKNChftzIxFCYZhZZofwf+TxeFHpBagXMJiM/hc0iYTEwv3FwQDrgdx9bg+cBnwe/kU6VAjHNXAH5N2CVQfm1dW/ENkKNwryMcTkKANHfv7mWS79rphIZ/SmDyGaVkXGqVOKPJ509bW6ulp6bYfCgqqJnhmNeHhmVNSrVw0ZAmwm87wM8yAtspVwf25UpfDjUuQ8qD5ErWQRtg66IvzahXB/ccTP2BXzL1h+2IOCW+dwQzArsXPcIsJHF9mKyK6MP+C8BfR9SQ4n3Bm92YcQIoK8g8/jSTFuevbsmbS7STBCcY0GV1bU9BkM149T0o8f53G50rLDNTU1hw8fJq5OBkK4shVY06dGMFxxjyO0IRGsxAV7Jg/yOpRAi6BLfK507PDjx48BVZZ6w+jrtIi/uJIsU6SVR0yB7UW0hSQVe569LUyCe5aIvBG8OXrkiFS4lpaWIqsRfqgiA2nkyp4WEUm4kn6HiGPx29SNtU+QQZIGAeSySJlE/GVHEOhrWloan82RJFfqVbL8fNm/cSWguc6YMQP6OnXyZMIVYScAkHbaTUeWYw2SHIQqMMuIY0kaereqNPXc7ht3i8hOECQjbLn/8G6ziwK6bw2yT9LQU/WkEmgRXiHMwW9LLbfCXcxlIDiBQwcP4v6WGFfsBXdKRUWFJE6vzQJHPnXqVLaCwuTwcNKLCompdsB3CE0HRJmCMdaMjR9qPZ5P+hUfy9+/av9cJc+/G476fygGT4mI13Ic2yj4V6QoAjoAFl5k5HndM3BFjkRSSQSrSEBDV7k3bblt2sVcBgKuKQdSkMdLIB7GvnJycrKzs2UWJTUVwlWhi0L4hImEK9J8eMHMS0eQGIxbOezUxTQyBgDSUKipgB4vYm/WlqRTG0nTEkiQ3BG/0EuRReZAUHeq3SptKfIc3BBYA/wLdkxu2nLbtIu5DIR6V2X/fh6HKwGuCJGqxRuyTXxp1FfW+NCw+noqTCXdhmO2hqKWdQN/mJ84CVyhPaRPmoA2y9Az8uAlq/AYNE/V+x+e8+3JozeRRUagx6Sl8PeDseZj/iug2xGX7Ylq2nIraNLFXAb1AK779+0DVxm1D+fl5W3cuHHbtm3Xrl2TRkhFcY2IYLNYYeNCCdcZm0bDzMI7ok7h4QT02En9ZhiT8jgHaDPsJFwv9UjuNDVKG+hCm42DfyOJv8gisyFpmIUpJlqIe6XZltumXcwlftVNBVz37N7DpeImKbcj1tXVJyYmdu/e3da2p5GhkZ2dXW5urjjHa1bANSJiKkuBNXbMGMKVPJMZGG3OH/EZCVtA2iVSj5QnT+Kyr5x0idTXC/wxNili3aFFXgsdSAstfkUWhY+l6PG3hTumkPErEH9R7b0XUpq23DbtYi7xq24q4Jq0M4mKm6TdD+bChQs9evQIDQ29evXq7t27u3Wz9PX1ffpUwt3hiR1mKSiEjBxFuEJgDFGh49d4kMXpm4JR0cQeIqgBThXvLyM2BE5Z7w+jXfHoHrQTU7dQDoIjkUXmQKR3f/DywTDIFuNYmIc1rqt/0bTltmkXc8lecrMCrjsSE7kKUuaKNHnt77/rG+iTzhIwYnv37jUwMMzIyJBs+yKtrxHgGjwqmOEqoAdBZFqFoLVMRxYB3ThMhrEDaeIawDL59FbG6oosEkFJpEbnb54lBRAGkx4qzbbcinQxl4GgVrcnJEhdX8lrqUpKSiFjxlCDxLx58+zZs2Huw2JiYiTbGgWukRGRsMPBQvr6EQrFdVuC1PUVcuTIER0dHS6X6+rqev8+9fBky5YtAwYMkGwIDa5TpkxBPBzgH0Ce9eKqSktLJXgIuRBwTdiWwCZcH0uT6/Pnz9esWWPetau1lfX161Qvshs3btja2p47J8lXQsF14sSJ0NcAP3/C1c/P75NPPjl2rJ3RygfMxcURimtCAovFGtB/gJgdj96d58DklpSUACdxY1gMDw+fM3eOBDtRYJ8iXCMjI8G13Y/0+Xy+u7u7pE5PZsJwHTJkyDPZf2flYsFFV1e306dPSyp6et3I1c/Hl3CFCwfXoqIiBORnz1JhDiwHGQjuzJkzmzdvRrqFGSxWVlYuWbKktrYWNwEzCCAch5OTE/aJMywuLpYX9SVc2Wy2m5sbrlecXbWHKxT3DC2Sqi+ir2wWy8vD88UL6hMNsbGx4Hrx4kUvL68ff/wRRHEXf/vtt/jvhAkTvv766y5dunz66aeZmZmoCJSE78ca6Ojt27cnT578+eef//rrr9bW1o8ePcI8YgKJnKe0hdFXDw8PUg/tlnaOK02+GCPOgYWFscPenl7kelasWAFa+fn5/v7+6urqw4YN+4QWkJ4zZw6Z0dTUHDdu3IYNG777jmrpxW0OotBjVMo333wzfTo1yHBNTc2gQYOuXZP8wKPSEIYrwgsx84IOMV74q1evRbj+/vvvgJeTkwOu33///RdffIEEF2uysrLA9bPPPnv58qWFhQWYQbP/+1+qpfc///lPVFQU2eFvv/1GuMqXgOvOnTtxgwYFBYmZSXYIrkw87DliBOEKDwqKcOHgipk+ffokJydjJiMjg+hrVVWVmpqap6cnNPuXX6iW3q+++mrx4oYBo7FGTrkmJSWBa3BwsJg+rqNwDaf967ChQ2trqXgBty3gnTp1inDds2cP4Xr06FHCFdjgOKHWKSkpWDxx4gR+mWFuf/rpJ+g3ZvLy8pBtC4+MAZ1eu3btwYMHYevKy8sRXsE3owyis9mzZ8fHx6elpeF2+SD1wOgr/IuYz1c6BFfYHFwJuLoPHUa4krhp06ZNISEhmKmoqIBN1tbWJlyhjgimdHR0EAAjfoZDQhlYYybWgE02NqYe/iD5hvcVDgV0dXXHjx8/adIkuOeTJ09iwytXrpB7wtTUFIoC8IqKis2ep7QF57kriWr3nzxpUifhOnbsWFpfhz1vzFVu3boFbMhSFi1aJFwYXGGBkd4xlurevXu4A+7c+eu19iNHjhDdRe0gERLe/MmTJ9gtNJh8UAoqK6AHqELJp0+fYrc4GTG/NdVuobju2oV6mD5tuphhacfi6jbE9Z15GzQbFljMtL1jClgm702Gvs6KnikQL9voWFwHDxrMcIUOnT9/HgGwSASBTAba+SFOU+oCrvv27QPXubPniLmrDsQV8fDgQYMYrnp6ekhv/vnPf9rY2LTibNoUN3bwhieKazLFdcE8cYdS7VBcFQYNHEi4ZmdnI5aJi4sjoQ2ixGY39PHxUVBQeP8DdfB2Y4rrXprr/E7FlTWgX7/ntOOcN28ecJLRypG0kMRj+fLlCKbKysqYkS68vLxUVVVDQ0MROu3YsSM9PT01NRVlBO9qN4bikmIdSjopVxarf99+JCAKCAj49ttvyX+RflhYWCDLBOmEhAQoXJcuXVAMmH/77bcvv/zS0tISbtjIyAibIMPBfYDItvV2Y+DHfz/IF2VakZa4IpfDjdumzKdDcEXWMXr0aOhr/34NXH19fcGM/NfR0VFDQ6OkpAQkEEl5eHiYmZlBZf38/KCs3bp1I8Xs7Ox+/vlnrEcxRB+ttxvn5+cPHjy4o43ZR/vXZLYCS4TrxYsXx4wZ06Yvh3YUrqNGjUI83K9RXyMjI//2t7+RZ3YIoLp27ZqXlwdgN2/eHDp0KBSObDhkyBBhrlBrVA2KxcfHy2O7MU5+7569rC6s+W/HTSdOnMBd2KZxlsTiWl9fDzW6cOECchJx7n3CFfrax6V3TQ3V2XH37t2kdRA36WeffQYPWlhYiDX4dXBw6N27N9lw4MCBwlyh1kRf//jjD3lsN6b7me6Evs6dM1d4/YEDBwYMGCAjrghMoqKioUk6OjqOjg7JycntfgTRwJXFcnF2IVzxC2fp6uoKVSOA4TKhwVu2bIEWjh8/nmzYv39/6CiZB9dPP/105syZKH/jxo3W243Xr18PkyB8Ds02Ecu43Zj0HwbXmFlvdWvdtm1bnz59Kh9UtrRhU2kn18ePH0+aNAlQoUmoeisra11dXVRi+7SWscO9G7lCtm7divwVVKysrMga2CIsgi4sBFkDm/yPf/yDOB5wVVJSAlr4XQEda7TSbuzm5ubs7Cx8OU2biJtd2b7qek9B7SVu307G2RBaSX28D1yr3tbX1qu6PVxhfnH/DnVzy8jIeFlfjzjt8uXLjg4Orm5u7fsqOGOHHR0chftrVVVVZWZmMlkKZrZv3y78wgFyGFwzCRTBNSgoSLh7ZivtxiAt8snbZpuIZdxuDFSbN21GPcwQah+mevsuXQbLJNwHFKeE2JDRgabSZq7Uu3wpKd7e3gUFBUzkjZWHDx9GpnHw4MF2qCxjh+3t7NrdbxbhFeLk9m3bQQT1uX7dOnCdPm3aX9/pe/167ty5iCSEdaakuMTJ2Yn0EG1W2swVmQbcTE5Ojkg6BWWCsYqOjm5HWx1jhx16tp8rQi04gvZt20EEVbpq5SpwnTp1KsMV1hExwaBBg4QNDMLVnj17Iv9paVdt44o038fHZ8eOHc2GSIVFRbl5ee3WV3DtZWfPcMVFQvvFHPT40aNHUh3ZUbKCS46Li1NQUEDCzVQjXAaCGLehQ4Xv+Nu3b3fv3r2Vd+DawBXWfNq0af7+/kg2SkvvwPfA67w9vPc7nHlL0sBVgWVn2/Pxo4avR8IqkL5qS5cuJWvaGm/DCSHSwR5k8Ck2iciff75eunQJuI4PG89ULCo5ICBg+PDhwt60tLQUSfyZMy3esm3giogfHtTGxqZXr16Ojo5IQnBbnTwhga9XMFzte9g+ornCc4BHjx494DUR5Qro90c+//zzNvUUP3Xq1N///nfSI0LMM5SNwIUtXLgQXMPCwhiuuDs9PDxgJoUfOZeVlUFf09NbTEDawBUHwA2yZ/futWvWLFiwYM6cOcuXL4ejlSDXnj16EK6kdRd6Zm9vj8QGF4zIlnRrQhCOAgjT1q1bV08LTgOZz969e5v65iVLlpD+5Zjv+B3EcXpzZs9W6KIAw8twRYXAucJMMnmBgA71cdMfPNRilNqePIcaN4UWSb27Trgib7Pv2ZNwRUICHosXL0YGtWjRIsREBgYGWIOIF2YZTgi6+8MPP8ycORN38Wefffbjjz/++uuvtra2IntOTEwk/ZBxth2/gzhOEleEvGDMmDFM3T548MDJyQn1I9xTHDHUkCFDTp5q0Vh2lPbhkSNHgmtPm+7V1VQ0j6syNTUFMKzHhUVGRlpaWoKQgL6r/vWvfy1btmzChAkwUFjz5Zdf9uvXb9euXZ9++qlIVnrgwAHSX1UuOoiD6/QZ0ymuISEM17t378Lkjh49WviFKFQCLqqVtxo7BFfYUtgZcO1ubc3k/lBEWN3vvvuOy+XCBK1atYpwxXViBsCgo6mpqQKaKzKBY8eOMSaXkePHj2MlfmV+Te0RBIbUuDgsBWF9JSESPG6bviPXUbj6+fkhz+lh1cAVefDgwYOBEyYXYPbv30/e7BDQL2pi5quvvmL6KYIrzBSyL6wXeWsWsR5WIl+S/UW1Q8B10qRJ0NeQMSF/NoYCcEyoHOaLjO8pn8ydO7eVZgvZCMO1u7UN4bp161YY4ZiYGNJxAjERAYxrgzkC1M2bNx86dAjRsoDmyuPxkBchkhS5eKKvqBS56CAOruPHj6f7+49iQjwo7t2yslaaDJsVKkGEQZPCSbZBGK42VtbM0yhULslf1dTUEDKAAebBEv/CTf31118D/IwZMwQ0V1VVVSwCtsieyVs9SP7kooP4q5cvx44dy+FwJPAeB255BCCSOrP2SSNXtpVlt0qhsfwQ5gAkuVWRZSGfYbwvQlzS+0lAc4URbrYBkowmR5rvO34HcZwMNBV2uJO8n8PoK7i2dYxG4IGSTZ48WUrnJksheQG44vd1q19ae6d0FK6Ih2mulgzX92wfRjGEVDdv3mypANSu9ZSg4wi4BgYGUu9JBgZ2hvckcT1BI4PAtZvFX1zFbx8W0MEz3DN28sMPP0jvO2ySEsIV+hoUFCTm17E7Cld4FA6bbdnVgowmJJH24fT09J9++klZWRlmTS6yWNgtX19f8l5zJ+E6ClxZLAtT83v3qJGgJdI+jFj3m2++wY1CYmnYaoRg5L7pmIJr8fb25nDY0NpOwjWY5mppBq73BJJrHybNisbGxkhmBHRzB5/Pl/0FvqfU19XB+0BfOy1XSbUPC2gDTnqekvmO3M+0rq4OV8Ris4I6K1eBhNqHsTe4WGh5R+va36yAq/twd8RNnUhfRzH+leIqkfZheGUrK6vvv/9eXp6r43pd3VyhrwEBAZ2EK+nfxHCVSPswokpsYmdn17t3b3V19XPnzo0ZMwbmmilQXFzs4uLy6NEjV1fXU6dO4VhLlizBsXAyZWVlzs7OcAHDhg2TWSBd+6LWzc0NXGl97RT5K+Ha1cwcZpasFLN9GHcG2RzIzc3NtbW1kfb8+9//jo2NZcokJyfDJSNjRsgN3tbW1kgzxo4da2hoiFAc25aXl3/xxRfYlWzqAfoKK6XAUvDz9+sk+kr8q7mpWdmdv95dFLN9GEmwyMrq6mrhccyg3CTtqayshNGuoQUFSOMU+RdUVma9Z54/fz540CDc37i9XnaK8dZewUKCa1cT0zttHHa4M7UPP6t5Nrj/ALYC28/HV2Lj6J09e3bq1Knz58+fMmWKjD/qSN5rbh/Xd7YPy5HAWvTv24/DorhKTF8RX6ioqNjb28MPyTgreJvrnXdv0EkFvqZfn77vyRXeoaSkBMFgRkYGfkXejPrrO2bI80JDQ0UGwZKNkPfV26evnUlAAaE7zdXn5ct3cEX0AGdsbGyso62NX+T6wqHWJ2QBIR/p541AX7rn3pww/YfNTEw/wmH9GXny+LGLkzO4+sK/vosrjBySMTL09uVLl0W6AFDfMRPQL/AmJSWFhYUhTZTiibcgTP/hrh83V2TSTg6OHDZHAv4VCQYy/V9++QVBdkhIyLJlyyR1lu8vDFczI5Pi4mLZn0AHkUcPH1JcKX31ETceZrFY+/fv79u3LxbAmHmILUthuJoYGXfAYZVkJsibHXs5UFy9xc5zVFVV4XXJCw5Dhgxhhk6RpZB+AjRXo5s3P16u8JGOdvZU3OTr97JevPamGTNmfP311zdu3DA1NcXMzz//bGFhIeOPTpF+a+BqbGhIOjM320gr0rrb+aSystK+hy24Bvj7i9uOiLyeNLYxr/WIucd2CMPVxMDw2lXqFZpmG2lFWncF9MfpduzYMXTo0JjZMSkpKYjn3d3dly5e0r5hLt5D3jQVCe4dXG1proH+Aa86Qbv/X/qqb1BUWETWNG2kFWndJcVGjhzJ4/ESExNxgxYUFNjb2akqq8QujJX43fn8+bOzZ88eOXLkwIH9e2jZt29famoq1T/5/n2JvFpYcb/C1tqGcO0Mz3MYrkZ6+kjF3n9D2BgPDw8lRSXygJ2MaaKno2tuapor0S+tCehXicMnTuw/oL+1tbWOjo6SkhKfx8ctpaKi0qdPX4m8qXe/vLx7N2uKa+d4/irM9VIbucLwKispHz16lKxBthYWGsrjcINHjXoh9CKwROTVq1c1NU+RYefl5SHvT0hIWL58OTLDAwcOiPk5OSL37t2z7mbVqK+dgqs/zRWqduH8hfffsClXyMmTJw309LU1teVueJi7ZWXdLCy5CsS/dgquRF8NKK7n339DcEWorKysLPzlSajOxAkToLIB/gEvasX6GpiM5c6dO1ZdLTuVvrab6/Dhwymux//iCi+bmZGpr6unpaF5OitLCucrLYGF72rWlc1iwb9+1PpaV1fn4+ODyAW2V3j9k8dPcMtz2JzwieEyfpYsjtwuKTEzMe2EcZNe27n6+vmqqqqSlwAYQdaxc8dORZ5iV/OuJSUlkj5faQnhymOxgzoHV1xDQEAgzVWnTXETboiAgADoqwhXAd3X0LKrBZfN2b59u0RPVopSfOuWmZEJ9HXUyFGd5H26YOq5OluCXKHK48aMxT69vbzr62TaLNpuuXXzpomRMc559KhO8V4zuIaMHs1VoPzrxYKC998QXAMDA2CHmw6CSL5Fo8jhGRkaXe/Yw/swcrORa3Dn4AqbExoaylZgGerqXbp06f03JPqqpqZGvsktIvCspiamXA53x45EuXiP48b1G4b6Bhw2e3TnGIeA+k5oOPWdUFzVlbdfsGldCFfoK/kGu4jU1taOGDGCzWaHjQuV/cOMdsj1a9cprqxOxDUiIgL3qZGBUZsGHaLtcCC4NjtcAaLiBQsW4HZx7OUgMmhIx5RrV68RrsEjR3UGrgAwa9YsADAzNS1pSz8YBEdBgUEaGhr5+fnNFkhLS+NxedqaWgUX2hCOfSgpKirS19VD2h0SPPrPTsAVzm/BfEqxdLS0YmNj169fv3bt2lWrVsUzsmLF8uUrVsSvaFiKX7Fm9er169atXr3a3s5eS0vrQgvYCgsLqWcvXN7+ffuEXWxNTQ1MN7wyFP3cuXO5ubn5ebg38rGfgoIC+PjLly9j26tXr167du3GjRs3b9xA4gSHffv27TulpXfLysrv3SsvL6+4X1FZUVH14EHVgyrIQ1oeP3785MmTp0+fkhdDnj179vz581pMtbV1L+ow1dfVwdK8xNQor2jBoXW1dcB1zOgQMR/8dRSuS5YswfXAF/J4PD4tmOFxuXwuj5kU8Z+GeS6ziAxVR0v70uXmo60HDx5YWVujzNLFi4VrKvdcrqGhERRdQ11DS0NTR0NLR1sbdaqnq6enq2ugp2+or29kaGhibGJqbGJuatbVzNyqq4WVZbfu1jY21jY9bLpjsu1h28vOHpOzk7OTo5Ozo3Nv5959nFz69+03sP+AAf36Dx44aMigwW5DXN1d3Ya6urkPHTbcfbjHMHcvT08fL29Mfr6+fvjz8YXVCQoMdHN1VeLyET+OFRofsX3SIbhCsrKy3N3dBw4c2K9//z59+vTu3dvF2dnFwdGhV69edna9bHv2RFUCkYVlN3NzC1NTc9S4oZGRgaESXxFcW3rDFYZ6wICBPBZnwvjxwqHT4cOHEUVTdw6Hy8PE5XI5HB6HA9+Gm4DPRu1yFKkZemKxMXEV2DwWGwWwN1Q9sjK2ArXIplZy6N+3Ji5uUqoMPbH+KsymD4Fj4ZeaqINSE71I7RwzkVMjOglXhAmwXdXV1dCwiooKmLi7d++WlpbC9CFbv3H9OsgVXi4suFAAg3ku51z22eyszMyT6emugwfraOsgkmx2t7AEgQEB4OHl4flcaLxtmNZ169bBmK9auWrF8hXL4+IWL168KDZ24fyFC+bNnztnzqyZs2ZGR0+fNn36tGlTJk2eHD4pdFwo9FVVWQW7gm4F+gf4+fj6QOs8PIcPc4c6DoEMHgxN7d+3f78+faG7Tg6OvexxX9r3tLWFfltbWdlYWllaWFqYd+1qbg5LYGxkbAjToKevp6Orp6ODOADZNoxwfn6emPXZUbi2T6CCYaFhqJdbLfdinDxpEltBwcXJpbr6rR7xQP5nc/JaRF69Js7v6dMnISEhiOwQtZJxaOoa5cWLF7WNAm/6rOYZnCtuU/haON0HDyor7t+/d+9e2R245lL4aXhruO2iwkI4cnKn5p47l5OdA38PH4+diJ9tyzdXVPeUKVOMjY1beUsAERmri4K1RTfmjen2CSIgby9vc3Nz5r3bjizyzRXqFBkZaWpq2gqzlfHx7P8qGOsb3Lr11ruUzagm0c+3BcvUiPevXz+orERA1KN7DzJCZgcXuecaHU19+7CV0bZWr1rF6aKAoDcjIwP+G2aTWGAkFampqUlJSZs2bYqLi5sVM2vKlMmjRo3y8fHx8vLyGOExYvjwESNGeHp4+Hh7+/v5+3l6aaupw523+8tNshS55zp37lwbG5tWxj5E7gv/qqaiihQ2LS0NCRVib3AldOGh4c+ACnsoKytDpoqUKScn+9ixY3t27964cePy5csXzJ8/M3rm0EGDVXmKY0LGyEWTpHxzBZh58+fb2dm11BEc8BYuWIicRENFNTc3F/cB1DQ0NLSVL4AJbdzQDZzgnz93rpqiUtzSpXLxCEG+ucL/wb86Ozs/aTLMmoDS5j+zs7MdHRwsu3YdGRREbDUg7du3b8eOHW06EOLbEUOHaaqrHxfqSNWRRb65wll6e3sPHDig6fD3QJ6almZlZeXl6Xn1ytVnNTXCX/IT/hbN+wjSEmQ4dra28tKrRr65Pn/+fODAgUh1RHweeO/du9fa2jo4OBgpkJiWE5vv2pWkqqoaFhaGI4p3yjIS+eYK8+jo6Lj0bZ9XXVW1atVKS0vLiIgIiTyeI190BNeE7dvlwrkK5J0rwiUEw/PmzRP66lcVcJqZmc2fP18i71ZAbt0qRirVvXv3q9faMKr1hxX55lpZ+cDCwiKq8WPCUKa1a9e6ubnt2bOnrR60JcE+t2zZoqSkOC0yUl6MsEDeuZaXl5uamoaHhzP+tZIWSX0QUdDYmUZTUxMZrbwYYYG8cy0rKzMyMoLzk15bQX5+vp6enouLS1s/APNhRb65lpbeNjAwGBMyRkoDJ1DtHvPm8Xi82IWxYr5ALmORb67Fxbf09fVHjhwp/A1NCUpVVZW9vT309fz583JkhAXyzrW8/J6zs9PUqVOlZIcPHTqkqqjo6+3d1s/+fXCRY65VVdVbt25BVrNy5crs7Ow7d+48efIEgCWlWK9ev4KF57M5G9atk2AgJhuRV64wvDExMTY21tra2mpq6vCyPW1t3d3dZ0ZHJyYk5p3LRVT84kWdODxKSkrMTM2MDY3l5fMAwiKvXG/fvm1nZ5eQkLB79+5Ro0ZZWVmpqKhwuVw2i6WqrGJmYtqnd59x48atXbPm2NFj169de/jwIW4F8njuffaPYtu2beOwOYEBAXKUtjIir1yPHj2qo6tLXstBMAzMBw8ejIqKcnNzMzczA1oeh8tWYHFYbDUVVVNjk769+0yeNGnTH5tOnDhx88bNx48fN2Jufv9gOWLEcA6Hm5SUJF8RExG55Aoe69at09LSLiosFF4PAOABxidPnFyyeLGPl3c3C0tw5XN5HLrXJ5fNUVdTMzMydnJwGD0qePmyuJQDKUVFRdXV1eRbsEShIRfOX9DW1LK0tITb/lCXKY7IJVdEp+Hh4Ug/WurmL6AZQyPv379/7ty5jRs3Thg/3tHBUVdbR1lJicuhuvtyFVg8FkdVUVlPR9e6m9VQt6HhEybExsYmbNuWlZk5PXIaboKIqVNl9tEGyYr8cQWwvXv36ujoAI+bq+v2hO0gV3an7OnTp6284w1dhO2Fo01NTY1bFhcUGNTdxgYaSb00wGro+a1I9QXnKvP4GsoqKjw+uM6YPuOZUK9jORL54wo5nZVlamyswuUpc7hqikqaqpRpHdSvf1ho6KqVK48dO3b9+nUESi1hfvPnm/q6+urqqoKCgl1JSTGzZg0ePNjMxERFSYXuzk8xJn32VZSUjh45SvWIkTeRP67QV5hKdSUVZQ5PGerF5irSJMCDvLGjoqyir6vn4uQ8NiRkZXw8fG3xrWKY7mZzHuwN+KGUJcUlaWlpUOVA/wD7nj011TV4HI6xoREZr1HuRP64IvqNmj6DaBVtPIn9pH71tLRVlJQBA8Ewh37dRREKraLazbzriOHDkdru2b0nPy+/vLy82dT2jYDqoka88oEDBxBXe3p41tVLpYVS2iJ/XB89ejzU1Y1HKygzASpFms1R4vIMdXWBk/hLVb4Sn03fASjA5YI6Qie7nj39ff1WLF+RmZFRXFxMWqlEMCMi09LUil0Y29JpdHCRP65Xr161sbJGsKNIvwpHvYZG56mcRg3m/6XKHCUOHROxG16FE3rZjQPGBjp6PXvYenp6xsyK2b9vf+HlQqgybDJMwtatW1Hg0MFDH/py2ynyxzU5ORlxrBKX6zF8eGRERB+X3op8PtJTDovFb4KW0CUqyxP9F4foOvViJJenoaJqqG/g5ODo5+cbHRU1aMBAaPa+fftq5bCxSSB3XJFNLli4kM/jqyoqIcOhviFzpwzBkZqKWgtcOQ042W/Dxq8CNaOroRno42dl3lWFp4jCRPX59JupSnxFuOr047L++pdERM64Pn/+3NvLG5Wur63DNErcvXvX1MSEzQISFuNx6XeEGyZCV1FBaKK54ldbRe2PdeuH9BuAfEmRfoVZyGJT7VOIvf/HVepyt+yurbUNat/Z3r6i8V0ruMM1a9Z4eHjoaOuwFVh8BRbIUb+Ik1lssqjYnIkm8ZSOhqYKj6/Y+F46M/Hotz/Ef8X4g4iccc3KzNLX1uVTI/T61wtlIHTLcO28efN72vYEp0ZXylHm8XnNeFZhruwGolSzYiPRxhgbdvj2bfno4C8i8sQV8DZu2Ai3x2dzYxcsbGoekXpWVlbu3bPHz9cXyUz4xPBNf/xhaWFBRntoiWuDu23insHVxcn5cXNv/nR8kSeusLdhoWHweWqqqqmHWsxAkIkiVwFgMiqAk5Mji8Viv53vUpMCPbWsyoosToCf///a/aUujx4+tLe3R3zUzbJb4dtP6FqS+nrqS1rKysp8Ho8ajkWBhUAaqqymokrHWWx+E7cqzHXp4g/wdU2JiDxxvXXrlrGxMYfDGTFiBPm6zvtIcXHxunXrBvTvD0VHmtvDxiY9Pb1/7770kDuslvSVirk43LRDqVK9IumJPHF98uRJRESEu7t7ampqmzouUZ/tGDaMNDNNnjT53r17To5OVG8Kdmt2WFdDS04b/QXyxRUCf9nKA7iWBMptZ2cHipoamikpKXfu3LG0sOSzOa2Eylhv272HyMdy5UjkjGv7BHgcHBxghl1dXcvu3r1YUGCgo0uPqNaisnJYLA+PEXIxlESz8lFwhdHOyMiIXRSbm5v3+vWfx44e1VBRpSOm5puU+fRod9HR0R/6xNsvHwVXIkzfw+PHjmuqqpIWCZ5C82i5XG5iYuKHPWFx5CPiysjTp09Dx45rsQWKTn5UlVWaHVteXuRj5FpXVxc+cSLdAtxiI5Spiam8DBHSrHyMXGtqajyGj6CbnFrMXx0dHOW0JyKRj5ErMqXejk6Ap8zjayKAas4OjwwKktMWRCIfI1f4V08PTyW+om2PHna2PflcHo9DdX2iXgtofOy6VE7GVWtJPkauAHblypU1q1cfTDkIrkp8vq62zsyoaCMDQ7ojI1tFUSk5OflDn6ZY8jFyJQK6FRUVpsYmYOni7JKZmWlkaMRlc6GvutraeXly+TidkY+XK+RpzVNfHx8DPf3f1/5+8uRJdTV1Dv1soJul5b179z702YklHzVXAR1D3bx588WLF6mHUmF+ybOBEe7DayX9bXYZy8fOlZG7d+9GRkZOmDAhOjo6Pz9froMmwf+4CkvDwO/ynN4w8j+unVP+x7Vzyv8HiRfetorNDkwAAAAASUVORK5CYII=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {
      "image/png": {
       "width": 200
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of running it on a page:\n",
    "print(f\"\\n--- Semantic Visual Element Extraction and Description Demo ---\")\n",
    "# Reset VLM counter for this specific demo if you want to ensure VLM calls happen\n",
    "global images_described_this_run_count; images_described_this_run_count = 0\n",
    "\n",
    "if PDF_PATHS and os.path.exists(PDF_PATHS[0]):\n",
    "    demo_pdf_path = PDF_PATHS[0] # AAG.pdf\n",
    "    demo_doc_fitz = fitz.open(demo_pdf_path)\n",
    "    # Try a page known to have a figure, e.g., page 1 (index 0) of AAG.pdf for Figure 1\n",
    "    # or page 4 (index 3) for Figure 2\n",
    "    demo_page_num = 0\n",
    "    if len(demo_doc_fitz) > demo_page_num:\n",
    "        # Define a specific save directory for this demo page's images\n",
    "        pdf_basename_demo = os.path.splitext(os.path.basename(demo_pdf_path))[0]\n",
    "        demo_page_image_save_dir = os.path.join(IMAGE_SAVE_PARENT_DIR, f\"{pdf_basename_demo}_page{demo_page_num+1}_demo_elements\")\n",
    "        if os.path.exists(demo_page_image_save_dir): shutil.rmtree(demo_page_image_save_dir)\n",
    "        os.makedirs(demo_page_image_save_dir, exist_ok=True)\n",
    "\n",
    "        processed_captions_for_demo_pdf_page = set() # Track for this single PDF's processing in demo\n",
    "\n",
    "        visual_docs_from_demo_page = extract_visual_elements_pymupdf(\n",
    "            demo_doc_fitz,\n",
    "            demo_page_num,\n",
    "            demo_page_image_save_dir, # Pass the specific directory for this page's images\n",
    "            processed_captions_for_demo_pdf_page # Pass the set\n",
    "        )\n",
    "\n",
    "        print(f\"\\nGenerated {len(visual_docs_from_demo_page)} document objects from semantic visual extraction on page {demo_page_num+1}:\")\n",
    "        for i, v_doc in enumerate(visual_docs_from_demo_page):\n",
    "            print(f\"\\nDocument {i+1} (Type: {v_doc.metadata.get('type')}, Subtype: {v_doc.metadata.get('element_subtype')}, CaptionID: {v_doc.metadata.get('caption_id')})\")\n",
    "            print(f\"  Content (Description/Caption): {v_doc.page_content[:300]}...\")\n",
    "            if v_doc.metadata.get(\"image_path\"):\n",
    "                print(f\"  Saved Image: {os.path.basename(v_doc.metadata['image_path'])}\")\n",
    "                try:\n",
    "                    from IPython.display import Image as IPImage, display\n",
    "                    display(IPImage(filename=v_doc.metadata['image_path'], width=200)) # Smaller display\n",
    "                except ImportError:\n",
    "                    print(\"IPython.display not available to show image in notebook.\")\n",
    "        demo_doc_fitz.close()\n",
    "    else:\n",
    "        print(f\"PDF {demo_pdf_path} has less than {demo_page_num+1} pages.\")\n",
    "else:\n",
    "    print(f\"PDF path {PDF_PATHS[0] if PDF_PATHS else 'N/A'} not found or PDF_PATHS is empty for demo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Orchestrating the Hybrid Extraction Process\n",
    "\n",
    "Now that we have the per-page visual extractor (`extract_visual_elements_pymupdf` or your chosen name) and other utilities, we need functions to manage the processing for entire files and then for all PDFs in our corpus. These correspond to `extract_elements_from_file` and `extract_elements_from_all_pdfs` that i will difine below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1: Per-File Hybrid Extraction Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-file hybrid extraction function defined.\n"
     ]
    }
   ],
   "source": [
    "def extract_elements_from_file(\n",
    "    file_path: str,\n",
    "    base_image_save_dir_for_this_file: str, # e.g., IMAGE_SAVE_PARENT_DIR/pdf_name_runid\n",
    "    # llm_vlm_for_description: Optional[ChatGoogleGenerativeAI], # Uses global llm_aux\n",
    "    text_splitter_to_use: RecursiveCharacterTextSplitter,\n",
    "    processed_captions_tracker_for_this_pdf: Set[str] # To track captions within this single PDF\n",
    ") -> Tuple[List[Document], Dict[str, Any]]: # Returns (docs_from_file, file_metadata_summary)\n",
    "    \"\"\"\n",
    "    Extracts elements (text chunks, image descriptions) from a single file (PDF or image),\n",
    "    handling digital and scanned PDFs. This is the notebook version of the logic.\n",
    "    Uses `extract_visual_elements_pymupdf` (or your equivalent) for visual parts.\n",
    "    Uses `generate_detailed_image_description` for VLM.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing file with Hybrid v11.5 logic (Notebook): {os.path.basename(file_path)}\")\n",
    "    if not os.path.exists(base_image_save_dir_for_this_file):\n",
    "        os.makedirs(base_image_save_dir_for_this_file, exist_ok=True)\n",
    "\n",
    "    final_documents_from_file: List[Document] = []\n",
    "    file_metadata_summary = {\n",
    "        \"title\": \"N/A\", \"abstract\": \"N/A\", \"source_filename\": os.path.basename(file_path),\n",
    "        \"is_scanned\": False, \"page_count\": 0\n",
    "    }\n",
    "    \n",
    "    # Reset VLM counter at the start of processing each PDF file if you want the limit per PDF\n",
    "    # global images_described_this_run_count; images_described_this_run_count = 0\n",
    "    # Or manage it per PDF if `extract_elements_from_all_pdfs` handles multiple files.\n",
    "    # For now, assuming the global counter is reset by the orchestrator for each file if desired,\n",
    "    # or the limit in generate_detailed_image_description is a global run limit.\n",
    "    # Let's refine: the MAX_ELEMENTS_FOR_VLM_DESCRIPTION_PER_PDF_EXPLORE implies a per-PDF limit.\n",
    "    # So, the calling function (extract_elements_from_all_pdfs) should reset the global counter.\n",
    "\n",
    "    file_extension = os.path.splitext(file_path)[1].lower()\n",
    "\n",
    "    if file_extension == \".pdf\":\n",
    "        try:\n",
    "            doc_fitz = fitz.open(file_path)\n",
    "            file_metadata_summary[\"page_count\"] = len(doc_fitz)\n",
    "\n",
    "            # --- Metadata Extraction (Title/Abstract from your notebook) ---\n",
    "            if len(doc_fitz) > 0:\n",
    "                first_page_meta = doc_fitz.load_page(0); page_height = first_page_meta.rect.height;\n",
    "                title_candidates = []; blocks = first_page_meta.get_text(\"dict\", flags=fitz.TEXTFLAGS_DICT)[\"blocks\"]\n",
    "                max_overall_font_size = 0; eligible_title_blocks_data = []\n",
    "                for block in blocks:\n",
    "                    if block['type'] == 0 and 0.03 * page_height < block['bbox'][1] < 0.40 * page_height:\n",
    "                        for line in block['lines']:\n",
    "                            for span in line['spans']:\n",
    "                                if span['size'] > max_overall_font_size + 1e-3: max_overall_font_size = span['size']\n",
    "                                eligible_title_blocks_data.append({'text': span['text'], 'size': span['size'], 'block_text': clean_parsed_text(\"\".join(s['text'] for li in block['lines'] for s in li['spans']))})\n",
    "                pymupdf_title = \"\"\n",
    "                if max_overall_font_size > 13:\n",
    "                    for data in eligible_title_blocks_data:\n",
    "                        if abs(data['size'] - max_overall_font_size) < 1.0 and \\\n",
    "                           not re.match(r\"arxiv:[\\d\\.]+\", data['block_text'].lower(), re.I) and \\\n",
    "                           not re.search(r\"\\d{1,2}\\s+(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)\\w*\\s+\\d{4}\", data['block_text'].lower(), re.I) and \\\n",
    "                           len(data['text'].strip()) > 2:\n",
    "                            title_candidates.append(data['text'].strip())\n",
    "                if title_candidates: pymupdf_title = clean_parsed_text(\" \".join(list(dict.fromkeys(title_candidates))[:2]))\n",
    "                if pymupdf_title:\n",
    "                    file_metadata_summary[\"title\"] = pymupdf_title\n",
    "                    final_documents_from_file.append(Document(page_content=f\"DOCUMENT TITLE: {pymupdf_title}\", metadata={\"source\": file_path, \"page_number\": 1, \"type\": \"title_summary\", \"importance\": \"critical\", \"parser_source\": \"pymupdf_meta_nb\"}))\n",
    "\n",
    "                abs_text_sorted = first_page_meta.get_text(\"text\", sort=True)\n",
    "                abs_match = re.search(r\"Abstract\\s*\\n(.*?)(?=\\n\\s*\\n(1\\.(?:\\s|\\n)|I\\.(?:\\s|\\n)|Keywords|Introduction|Motivation)\\b)\", abs_text_sorted, re.I | re.S)\n",
    "                if abs_match:\n",
    "                    pymupdf_abstract = clean_parsed_text(abs_match.group(1))\n",
    "                    if pymupdf_abstract and len(pymupdf_abstract) > 50:\n",
    "                        file_metadata_summary[\"abstract\"] = pymupdf_abstract\n",
    "                        final_documents_from_file.append(Document(page_content=f\"DOCUMENT ABSTRACT: {pymupdf_abstract}\", metadata={\"source\": file_path, \"page_number\": 1, \"type\": \"abstract_summary\", \"importance\": \"critical\", \"parser_source\": \"pymupdf_meta_nb\"}))\n",
    "            # --- End Metadata Extraction ---\n",
    "\n",
    "            # Check if scanned\n",
    "            digitally_extracted_text_sample = \"\".join(doc_fitz.load_page(i).get_text(\"text\", sort=True) for i in range(min(3, len(doc_fitz))))\n",
    "            if len(clean_parsed_text(digitally_extracted_text_sample)) < MIN_OCR_TEXT_LENGTH_FOR_SCANNED_PDF * min(3, len(doc_fitz)):\n",
    "                file_metadata_summary[\"is_scanned\"] = True\n",
    "                print(f\"PDF '{os.path.basename(file_path)}' appears scanned. OCR will be used for text.\")\n",
    "\n",
    "            for page_num in tqdm(range(len(doc_fitz)), desc=f\"Pages for {os.path.basename(file_path)}\", leave=False):\n",
    "                page = doc_fitz.load_page(page_num)\n",
    "                page_text_for_chunking = \"\"\n",
    "                parser_source_log = \"pymupdf_digital_nb\"\n",
    "\n",
    "                if file_metadata_summary[\"is_scanned\"]:\n",
    "                    parser_source_log = \"pymupdf_ocr_nb\"\n",
    "                    ocr_img_path = os.path.join(base_image_save_dir_for_this_file, f\"page{page_num+1}_ocr_temp.png\")\n",
    "                    pix = page.get_pixmap(dpi=OCR_DPI, alpha=False); pix.save(ocr_img_path); pix = None\n",
    "                    page_text_for_chunking = ocr_image_to_text(ocr_img_path) # Uses helper\n",
    "                    # VLM for full scanned page (optional, as in your notebook)\n",
    "                    page_img_desc = generate_detailed_image_description(ocr_img_path, \"scanned page\") # Uses helper\n",
    "                    if \"VLM description skipped\" not in page_img_desc and \"Error in VLM\" not in page_img_desc :\n",
    "                         final_documents_from_file.append(Document(page_content=page_img_desc,metadata={\"source\":file_path,\"page_number\":page_num+1,\"type\":\"image_description\",\"image_path\":ocr_img_path,\"original_caption\":f\"Full Scanned Page {page_num+1}\",\"element_subtype\":\"scanned_page_full_nb\",\"caption_id\":f\"ScannedPage{page_num+1}_Full_NB\"}))\n",
    "                else: # Digital PDF\n",
    "                    page_text_for_chunking = page.get_text(\"text\", sort=True)\n",
    "                    # Call your semantic visual extractor (defined in Cell 4.3)\n",
    "                    visual_docs_from_page = extract_visual_elements_pymupdf( # Or your chosen name\n",
    "                        doc_fitz, page_num, base_image_save_dir_for_this_file, processed_captions_tracker_for_this_pdf\n",
    "                    )\n",
    "                    final_documents_from_file.extend(visual_docs_from_page)\n",
    "\n",
    "                cleaned_page_text = clean_parsed_text(page_text_for_chunking) # Uses helper\n",
    "                if cleaned_page_text:\n",
    "                    text_to_process = cleaned_page_text\n",
    "                    if page_num == 0 and file_metadata_summary[\"title\"] != \"N/A\": # Add title context\n",
    "                        if not cleaned_page_text.strip().lower().startswith(file_metadata_summary[\"title\"].strip().lower()[:30]):\n",
    "                             text_to_process = f\"Document Title Context: {file_metadata_summary['title']}\\n\\n{cleaned_page_text}\"\n",
    "                    page_splits = text_splitter_to_use.create_documents([text_to_process], metadatas=[{\"source\":file_path, \"page_number\":page_num+1, \"type\":\"text_chunk\", \"parser_source\":parser_source_log}])\n",
    "                    final_documents_from_file.extend(page_splits)\n",
    "            doc_fitz.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing PDF {file_path}: {e}\")\n",
    "            if 'doc_fitz' in locals() and doc_fitz.is_open: doc_fitz.close()\n",
    "\n",
    "\n",
    "    elif file_extension in [\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\", \".tiff\"]: # Standalone image\n",
    "        print(f\"Processing standalone image: {os.path.basename(file_path)}\")\n",
    "        file_metadata_summary[\"title\"] = f\"Image: {os.path.basename(file_path)}\"\n",
    "        file_metadata_summary[\"is_scanned\"] = True # Treat as needing OCR/VLM\n",
    "        ocr_text = ocr_image_to_text(file_path) # Uses helper\n",
    "        if ocr_text:\n",
    "            splits = text_splitter_to_use.create_documents([ocr_text],metadatas=[{\"source\":file_path,\"page_number\":1,\"type\":\"text_chunk\",\"parser_source\":\"image_ocr_nb\"}])\n",
    "            final_documents_from_file.extend(splits)\n",
    "        img_desc = generate_detailed_image_description(file_path, \"standalone image\") # Uses helper\n",
    "        if \"VLM description skipped\" not in img_desc and \"Error in VLM\" not in img_desc:\n",
    "            final_documents_from_file.append(Document(page_content=img_desc,metadata={\"source\":file_path,\"page_number\":1,\"type\":\"image_description\",\"image_path\":file_path,\"original_caption\":f\"Standalone Image: {os.path.basename(file_path)}\",\"element_subtype\":\"standalone_image_vlm_nb\",\"caption_id\":f\"StandaloneImg_{os.path.basename(file_path)}_NB\"}))\n",
    "    else:\n",
    "        print(f\"Unsupported file type (Notebook): {file_path}\")\n",
    "\n",
    "    # Deduplication (from your notebook)\n",
    "    final_deduped_documents: List[Document] = []; seen_keys = set()\n",
    "    for doc_item in final_documents_from_file:\n",
    "        key_parts = [doc_item.metadata.get(\"type\"), doc_item.metadata.get(\"page_number\"), doc_item.metadata.get(\"source\")]\n",
    "        content_key_part = doc_item.page_content[:150]\n",
    "        if doc_item.metadata.get(\"type\") == \"image_description\": content_key_part = doc_item.metadata.get(\"image_path\", content_key_part)\n",
    "        elif doc_item.metadata.get(\"type\") in [\"text_table_content\", \"text_figure_description\"]: content_key_part = doc_item.metadata.get(\"caption_id\", content_key_part)\n",
    "        key_parts.append(content_key_part); key = tuple(key_parts)\n",
    "        if key not in seen_keys: final_deduped_documents.append(doc_item); seen_keys.add(key)\n",
    "\n",
    "    print(f\"Finished file {os.path.basename(file_path)} (Hybrid Notebook): Extracted {len(final_deduped_documents)} unique elements.\")\n",
    "    return final_deduped_documents, file_metadata_summary\n",
    "\n",
    "print(\"Per-file hybrid extraction function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2: Corpus-Level Orchestrator and Vector Store Creation Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus-level orchestration and vector store functions defined.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_elements_from_all_pdfs(\n",
    "    list_of_file_paths: List[str],\n",
    "    base_save_dir_for_all_outputs: str, # e.g., IMAGE_SAVE_PARENT_DIR\n",
    "    # aux_llm_for_summaries_vlm: Optional[ChatGoogleGenerativeAI] # Uses global llm_aux\n",
    ") -> Tuple[List[Document], str]: # Returns (all_docs_across_corpus, corpus_summary_text)\n",
    "    \"\"\"\n",
    "    Orchestrates element extraction from multiple files for the notebook\n",
    "    and generates a corpus summary. Uses global `llm_aux`.\n",
    "    \"\"\"\n",
    "    all_docs_across_files: List[Document] = []\n",
    "    corpus_summary_texts_for_llm: List[str] = []\n",
    "    \n",
    "    global images_described_this_run_count # Reset for each call to this orchestrator if desired\n",
    "    images_described_this_run_count = 0 # Reset VLM usage for the whole corpus processing run\n",
    "\n",
    "    # No global RUN_ID based cleanup here, caller of this function (main notebook block) handles it\n",
    "    # os.makedirs(base_save_dir_for_all_outputs, exist_ok=True) # Ensure base output dir exists\n",
    "\n",
    "    hybrid_text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300) # As in your notebook\n",
    "\n",
    "    for file_path in list_of_file_paths:\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File NOT FOUND: {file_path}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        file_basename_no_ext = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        # Create a specific subdirectory for this file's extracted images within the base_save_dir\n",
    "        file_specific_image_save_dir = os.path.join(base_save_dir_for_all_outputs, f\"{file_basename_no_ext}_elements\")\n",
    "        # No RUN_ID here, as base_save_dir_for_all_outputs might already be unique per run\n",
    "        \n",
    "        processed_captions_for_this_pdf = set() # Fresh set for each PDF\n",
    "\n",
    "        elements_from_current_file, current_file_meta = extract_elements_from_file(\n",
    "            file_path,\n",
    "            file_specific_image_save_dir,\n",
    "            hybrid_text_splitter,\n",
    "            processed_captions_for_this_pdf\n",
    "        )\n",
    "        all_docs_across_files.extend(elements_from_current_file)\n",
    "\n",
    "        title_to_use = current_file_meta.get(\"title\", \"N/A\")\n",
    "        abstract_to_use = current_file_meta.get(\"abstract\", \"N/A\")\n",
    "        if title_to_use != \"N/A\":\n",
    "            corpus_summary_texts_for_llm.append(f\"File:{file_basename_no_ext}, Title:{title_to_use}\")\n",
    "        if abstract_to_use != \"N/A\" and len(abstract_to_use) > 50:\n",
    "            corpus_summary_texts_for_llm.append(f\"Abstract({file_basename_no_ext}):{abstract_to_use[:200]}...\")\n",
    "        # Add sample content if title/abstract are missing (from your notebook)\n",
    "        if title_to_use == \"N/A\":\n",
    "            samples = [d.page_content for d in elements_from_current_file if d.metadata.get(\"type\")==\"text_chunk\" and len(d.page_content)>150]\n",
    "            if samples: corpus_summary_texts_for_llm.extend([f\"SampleContent({file_basename_no_ext}):{s[:150]}...\" for s in samples[:2]])\n",
    "\n",
    "\n",
    "    corpus_summary_content = \"Corpus summary: Default - no content or summarization failed.\"\n",
    "    if llm_aux and corpus_summary_texts_for_llm: # Check if llm_aux is available\n",
    "        try:\n",
    "            summary_input_text = \"\\n\".join(corpus_summary_texts_for_llm[:15]) # Limit input length\n",
    "            prompt_text = (\"Generate a concise overall summary of a document corpus based on these (potentially partial) titles, abstracts, and text samples:\\n\\n\" + summary_input_text)\n",
    "            if len(prompt_text) > 30000 : prompt_text = prompt_text[:30000] # Safeguard\n",
    "            resp = llm_aux.invoke([HumanMessage(content=prompt_text)])\n",
    "            corpus_summary_content = f\"OVERALL CORPUS SUMMARY:\\n{clean_parsed_text(resp.content)}\"\n",
    "        except Exception as e:\n",
    "            print(f\"Corpus summary generation error: {e}\")\n",
    "            corpus_summary_content = f\"Corpus summary generation failed. Processed {len(list_of_file_paths)} file(s).\"\n",
    "\n",
    "    # Remove any old corpus summary and add the new one\n",
    "    all_docs_across_files = [d for d in all_docs_across_files if d.metadata.get(\"type\") != \"corpus_summary\"]\n",
    "    corpus_doc = Document(page_content=corpus_summary_content, metadata={\"source\":\"corpus_summary_nb\", \"type\":\"corpus_summary\", \"importance\":\"critical\"})\n",
    "    all_docs_across_files.insert(0, corpus_doc) # Add to the beginning\n",
    "\n",
    "    print(f\"\\nCorpus Summary (Notebook): {corpus_summary_content[:300]}...\")\n",
    "    print(f\"Total elements extracted from all files (Notebook): {len(all_docs_across_files)}\")\n",
    "    return all_docs_across_files, corpus_summary_content\n",
    "\n",
    "\n",
    "def create_advanced_vector_stores(\n",
    "    docs: List[Document],\n",
    "    text_embedding_model: Optional[GoogleGenerativeAIEmbeddings], # Should be global 'embeddings'\n",
    "    chroma_text_persist_path: str, # e.g., CHROMA_TEXT_PATH_NOTEBOOK_RUN\n",
    "    chroma_image_desc_persist_path: str # e.g., CHROMA_IMAGE_DESC_PATH_NOTEBOOK_RUN\n",
    ") -> Tuple[Optional[Any], Optional[Any]]: # Returns (text_retriever, image_desc_retriever)\n",
    "    \"\"\"Creates Chroma vector stores for text and image descriptions for the notebook run.\"\"\"\n",
    "    text_retriever_vs = None; image_desc_retriever_vs = None\n",
    "    if not text_embedding_model:\n",
    "        print(\"ERROR (Notebook VS Creation): Embedding model not available.\")\n",
    "        return None, None\n",
    "\n",
    "    text_docs_for_vs = [d for d in docs if d.metadata.get(\"type\") in [\"text_chunk\", \"title_summary\", \"abstract_summary\", \"corpus_summary\", \"text_table_content\", \"text_figure_description\"]]\n",
    "    # In your notebook, \"table_html_content\" and \"other_element_text\" were also included. Add if needed.\n",
    "    img_desc_docs_for_vs = [d for d in docs if d.metadata.get(\"type\") == \"image_description\"]\n",
    "\n",
    "    print(f\"Docs for text VS (NB): {len(text_docs_for_vs)}, Docs for image_desc VS (NB): {len(img_desc_docs_for_vs)}\")\n",
    "\n",
    "    # Text Vector Store\n",
    "    if text_docs_for_vs:\n",
    "        # Caller (main notebook block) should handle rmtree for these paths\n",
    "        # if os.path.exists(chroma_text_persist_path): shutil.rmtree(chroma_text_persist_path)\n",
    "        # os.makedirs(chroma_text_persist_path, exist_ok=True)\n",
    "        try:\n",
    "            txt_vs = Chroma.from_documents(\n",
    "                text_docs_for_vs, text_embedding_model,\n",
    "                # collection_name=f\"text_coll_adv_nb_{os.path.basename(chroma_text_persist_path)}\", # Unique name\n",
    "                persist_directory=chroma_text_persist_path\n",
    "            )\n",
    "            text_retriever_vs = txt_vs.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10}) # k from notebook\n",
    "            print(f\"Text VS created (NB): {txt_vs._collection.count()} entries at {chroma_text_persist_path}\")\n",
    "        except Exception as e: print(f\"Error creating text vector store (NB): {e}\")\n",
    "    else: print(\"No text documents for vector store (NB).\")\n",
    "\n",
    "    # Image Description Vector Store\n",
    "    if img_desc_docs_for_vs:\n",
    "        # if os.path.exists(chroma_image_desc_persist_path): shutil.rmtree(chroma_image_desc_persist_path)\n",
    "        # os.makedirs(chroma_image_desc_persist_path, exist_ok=True)\n",
    "        try:\n",
    "            img_vs = Chroma.from_documents(\n",
    "                img_desc_docs_for_vs, text_embedding_model,\n",
    "                # collection_name=f\"img_desc_coll_adv_nb_{os.path.basename(chroma_image_desc_persist_path)}\",\n",
    "                persist_directory=chroma_image_desc_persist_path\n",
    "            )\n",
    "            image_desc_retriever_vs = img_vs.as_retriever(search_kwargs={\"k\": 8}) # k from notebook\n",
    "            print(f\"Image Desc VS created (NB): {img_vs._collection.count()} entries at {chroma_image_desc_persist_path}\")\n",
    "        except Exception as e: print(f\"Error creating image description vector store (NB): {e}\")\n",
    "    else: print(\"No image description documents for vector store (NB).\")\n",
    "\n",
    "    return text_retriever_vs, image_desc_retriever_vs\n",
    "\n",
    "print(\"Corpus-level orchestration and vector store functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. The VTIERP Hybrid Extraction Strategy\n",
    "\n",
    "1.  **Metadata Extraction:** Title and Abstract extraction using PyMuPDF heuristics.\n",
    "2.  **Digital Text Extraction:** For native PDFs, direct text extraction page by page.\n",
    "3.  **OCR for Scanned PDFs:** If a PDF appears scanned, OCR is applied per page (using Unstructured.io if available, or a placeholder).\n",
    "4.  **Semantic Visual Element Processing (as shown above):**\n",
    "    *   Extraction of visual primitives.\n",
    "    *   Caption association.\n",
    "    *   Extraction of textual table content.\n",
    "    *   Rendering of semantic visual groups.\n",
    "    *   VLM-based description generation for rendered visuals (up to a limit).\n",
    "5.  **Text Chunking:** All extracted textual content (page text, title, abstract, textual table content) is chunked.\n",
    "6.  **Corpus Summary Generation:** An auxiliary LLM generates a high-level summary of the entire processed corpus.\n",
    "7.  **Deduplication:** A final deduplication step ensures unique document elements.\n",
    "\n",
    "This comprehensive approach aims to capture as much relevant information as possible from diverse PDF structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Full Hybrid Extraction and Vector Store Creation (Notebook Flow) ---\n",
      "\n",
      "Processing file with Hybrid v11.5 logic (Notebook): AAG.pdf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7205abb5370949cf9962467359687324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pages for AAG.pdf:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished file AAG.pdf (Hybrid Notebook): Extracted 88 unique elements.\n",
      "\n",
      "Processing file with Hybrid v11.5 logic (Notebook): Self_rewarding_VLLM.pdf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf3efe0486e84424a5a0f36a6c3da087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pages for Self_rewarding_VLLM.pdf:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished file Self_rewarding_VLLM.pdf (Hybrid Notebook): Extracted 82 unique elements.\n",
      "\n",
      "Corpus Summary (Notebook): OVERALL CORPUS SUMMARY:\n",
      "This corpus details advanced AI systems focused on **generative tasks** and **problem-solving**. It includes **Analogy-Augmented Generation (AAG)**, which leverages procedural memory and past experiences for procedural Q&A and generating novel outputs like recipes. Additional...\n",
      "Total elements extracted from all files (Notebook): 171\n",
      "Docs for text VS (NB): 149, Docs for image_desc VS (NB): 22\n",
      "Text VS created (NB): 149 entries at /home/dipesh/WORKSPACE/LET's Learn/vtierp_project/notebooks/exploration_outputs/chroma_db_text/run_f6194d69\n",
      "Image Desc VS created (NB): 22 entries at /home/dipesh/WORKSPACE/LET's Learn/vtierp_project/notebooks/exploration_outputs/chroma_db_img_desc/run_f6194d69\n",
      "Global `text_retriever` successfully created for LangGraph.\n",
      "Global `image_desc_retriever` successfully created for LangGraph.\n"
     ]
    }
   ],
   "source": [
    "# Cell 9 (or later): Running Full Hybrid Extraction and Vector Store Creation\n",
    "# This is where your error occurred (NameError for extract_elements_from_all_pdfs_advanced)\n",
    "\n",
    "print(\"\\n--- Running Full Hybrid Extraction and Vector Store Creation (Notebook Flow) ---\")\n",
    "global images_described_this_run_count; images_described_this_run_count = 0 # Reset global for this full run\n",
    "\n",
    "# Define base paths for this notebook run's outputs (previously missing)\n",
    "# CHROMA_TEXT_PATH and CHROMA_IMAGE_DESC_PATH were defined in Cell 2 as:\n",
    "# CHROMA_TEXT_PATH_NOTEBOOK = os.path.join(EXPLORATION_OUTPUT_BASE, \"chroma_db_text\")\n",
    "# CHROMA_IMAGE_DESC_PATH_NOTEBOOK = os.path.join(EXPLORATION_OUTPUT_BASE, \"chroma_db_img_desc\")\n",
    "# IMAGE_SAVE_PARENT_DIR is also from Cell 2.\n",
    "\n",
    "# Create unique RUN_ID specific paths for this execution block if you want isolation\n",
    "# within the notebook for multiple runs of this particular cell.\n",
    "# Otherwise, use CHROMA_TEXT_PATH_NOTEBOOK and CHROMA_IMAGE_DESC_PATH_NOTEBOOK directly.\n",
    "# The original code had RUN_ID logic, let's replicate that for this block:\n",
    "RUN_ID_FULL_EXTRACTION = uuid.uuid4().hex[:8]\n",
    "IMAGE_SAVE_PARENT_DIR_RUN = os.path.join(IMAGE_SAVE_PARENT_DIR, f\"run_{RUN_ID_FULL_EXTRACTION}\") # Subfolder for images of this run\n",
    "CHROMA_TEXT_PATH_RUN = os.path.join(CHROMA_TEXT_PATH_NOTEBOOK, f\"run_{RUN_ID_FULL_EXTRACTION}\") # Subfolder for text DB\n",
    "CHROMA_IMAGE_DESC_PATH_RUN = os.path.join(CHROMA_IMAGE_DESC_PATH_NOTEBOOK, f\"run_{RUN_ID_FULL_EXTRACTION}\") # Subfolder for image DB\n",
    "\n",
    "# Clean up specific run directories\n",
    "if os.path.exists(IMAGE_SAVE_PARENT_DIR_RUN): shutil.rmtree(IMAGE_SAVE_PARENT_DIR_RUN)\n",
    "if os.path.exists(CHROMA_TEXT_PATH_RUN): shutil.rmtree(CHROMA_TEXT_PATH_RUN)\n",
    "if os.path.exists(CHROMA_IMAGE_DESC_PATH_RUN): shutil.rmtree(CHROMA_IMAGE_DESC_PATH_RUN)\n",
    "\n",
    "# Explicitly create them now\n",
    "os.makedirs(IMAGE_SAVE_PARENT_DIR_RUN, exist_ok=True)\n",
    "os.makedirs(CHROMA_TEXT_PATH_RUN, exist_ok=True)\n",
    "os.makedirs(CHROMA_IMAGE_DESC_PATH_RUN, exist_ok=True)\n",
    "\n",
    "\n",
    "# Call the _notebook suffixed functions defined in Cell 8.1 and 8.2\n",
    "all_extracted_elements, overall_corpus_summary = extract_elements_from_all_pdfs(\n",
    "    PDF_PATHS, # Defined in Cell 2\n",
    "    IMAGE_SAVE_PARENT_DIR_RUN, # Use the run-specific image save dir\n",
    "    # llm_aux is used globally by the function\n",
    ")\n",
    "\n",
    "# text_retriever and image_desc_retriever will be made global for LangGraph nodes to use\n",
    "# Or, pass them explicitly into LangGraph state if you prefer that pattern.\n",
    "# For notebook simplicity, making them global is often easier.\n",
    "text_retriever = None\n",
    "image_desc_retriever = None\n",
    "\n",
    "if all_extracted_elements and embeddings: # Check if embeddings model is available\n",
    "    text_retriever, image_desc_retriever = create_advanced_vector_stores(\n",
    "        all_extracted_elements,\n",
    "        embeddings, # Defined in Cell 2\n",
    "        CHROMA_TEXT_PATH_RUN, # Use run-specific path\n",
    "        CHROMA_IMAGE_DESC_PATH_RUN # Use run-specific path\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping vector store creation: No extracted elements or embeddings model missing.\")\n",
    "\n",
    "if text_retriever:\n",
    "    print(\"Global `text_retriever` successfully created for LangGraph.\")\n",
    "if image_desc_retriever:\n",
    "    print(\"Global `image_desc_retriever` successfully created for LangGraph.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. VTIERP RAG Agent with LangGraph\n",
    "\n",
    "Now, we construct the RAG agent using LangGraph to orchestrate the retrieval and generation process. The agent will:\n",
    "1.  Potentially transform the input query.\n",
    "2.  Retrieve relevant text chunks and image/table descriptions.\n",
    "3.  Rerank and select the most pertinent context.\n",
    "4.  Pass the selected text, image descriptions, and (optionally) actual image data to the LLM for answer generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdvancedRAGState defined.\n",
      "format_docs_for_llm defined.\n",
      "LangGraph nodes defined.\n",
      "LangGraph RAG agent compiled successfully as 'app_advanced_rag'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LangGraph visualization saved to: /home/dipesh/WORKSPACE/LET's Learn/vtierp_project/notebooks/exploration_outputs/vtierp_langgraph.png\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import List, Dict, TypedDict, Optional, Any, Set # Ensure TypedDict is imported\n",
    "from langchain_core.messages import HumanMessage, SystemMessage # Ensure these are imported\n",
    "from langchain.docstore.document import Document # Ensure this is imported\n",
    "import os # Ensure os is imported for path operations\n",
    "from collections import defaultdict # For format_docs_for_llm\n",
    "\n",
    "# --- 1. LangGraph State Definition ---\n",
    "class AdvancedRAGState(TypedDict):\n",
    "    \"\"\"Represents the state of our RAG pipeline for the notebook.\"\"\"\n",
    "    original_question: str\n",
    "    transformed_question: str\n",
    "    retrieved_text_docs: List[Document]\n",
    "    retrieved_image_desc_docs: List[Document]\n",
    "    final_text_context: List[Document]\n",
    "    final_image_context: List[Document]\n",
    "    images_for_llm_payload: List[Dict[str, Any]] # For base64 images if passed to LLM\n",
    "    answer: str\n",
    "    # Use the globally available 'overall_corpus_summary' string directly in nodes\n",
    "    # instead of passing it through state, or add it to state if preferred.\n",
    "    # For simplicity here, nodes will access the global 'overall_corpus_summary'.\n",
    "\n",
    "print(\"AdvancedRAGState defined.\")\n",
    "\n",
    "# --- 2. Helper: Format Documents for LLM Context ---\n",
    "def format_docs_for_llm(docs: List[Document], doc_type_label: str) -> str:\n",
    "    \"\"\"\n",
    "    Formats a list of Langchain Document objects into a readable string for the LLM context.\n",
    "    (Adapted from your notebook's `format_docs_for_llm` logic)\n",
    "    \"\"\"\n",
    "    if not docs:\n",
    "        return f\"No relevant {doc_type_label} context found for this query.\"\n",
    "\n",
    "    if doc_type_label == \"image descriptions\":\n",
    "        grouped_by_caption_id = defaultdict(list)\n",
    "        for doc in docs:\n",
    "            # Using caption_id for grouping image descriptions. Fallback for safety.\n",
    "            caption_id = doc.metadata.get(\"caption_id\", f\"ungrouped_img_desc_{os.path.basename(doc.metadata.get('image_path','no_path'))}\")\n",
    "            grouped_by_caption_id[caption_id].append(doc)\n",
    "\n",
    "        formatted_parts = []\n",
    "        for cap_id, desc_docs in grouped_by_caption_id.items():\n",
    "            # Get original caption from the first doc in the group (should be consistent)\n",
    "            original_caption_text = desc_docs[0].metadata.get(\"original_caption\", str(cap_id))\n",
    "            if \"ungrouped_img_desc\" in str(cap_id) and original_caption_text == str(cap_id):\n",
    "                original_caption_text = \"Ungrouped Visual Element\"\n",
    "\n",
    "            formatted_parts.append(f\"Visual Element Group: '{original_caption_text[:100]}...' ({len(desc_docs)} VLM description(s) retrieved)\")\n",
    "            for i, single_desc_doc in enumerate(desc_docs):\n",
    "                img_path_hint = os.path.basename(str(single_desc_doc.metadata.get('image_path','N/A')))\n",
    "                page_num_hint = single_desc_doc.metadata.get('page_number','N/A')\n",
    "                # The VLM description is the page_content of the image_description Document\n",
    "                formatted_parts.append(f\"  VLM Description Part {i+1} (Source Hint: {img_path_hint}, Page: {page_num_hint}):\\n    {single_desc_doc.page_content}\")\n",
    "        return \"\\n\\n\".join(formatted_parts)\n",
    "\n",
    "    # For general text documents (text_chunks, summaries, textual_table_content etc.)\n",
    "    formatted_parts = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        metadata = doc.metadata\n",
    "        source_file = os.path.basename(str(metadata.get('source','N/A')))\n",
    "        page_num = metadata.get('page_number','N/A')\n",
    "        \n",
    "        doc_kind_raw = metadata.get('parser_source', metadata.get('type', 'N/A'))\n",
    "        # Simplify display name for doc_kind\n",
    "        doc_kind_clean = str(doc_kind_raw).replace('text_', '').replace('_description', '').replace('_summary', '').replace('_nb', '').replace('pymupdf_meta', 'Metadata').replace('digital','Digital Text').replace('ocr','OCR Text').replace('content','')\n",
    "        if doc_kind_clean == \"chunk\": doc_kind_clean = \"Text Chunk\"\n",
    "        if doc_kind_clean == \"corpus\": doc_kind_clean = \"Corpus Summary\"\n",
    "\n",
    "\n",
    "        importance_level = metadata.get('importance', '')\n",
    "        header = f\"Context Document {i+1} (Type: {doc_kind_clean}, Source: {source_file}, Page: {page_num}{', Importance: '+importance_level if importance_level else ''}):\"\n",
    "        formatted_parts.append(f\"{header}\\n{doc.page_content}\\n\")\n",
    "    return \"\\n\\n\".join(formatted_parts)\n",
    "\n",
    "print(\"format_docs_for_llm defined.\")\n",
    "\n",
    "# --- 3. LangGraph Nodes ---\n",
    "\n",
    "# Node 1: Query Transformation (simple pass-through for now)\n",
    "def query_transform_node(state: AdvancedRAGState) -> AdvancedRAGState:\n",
    "    print(\"---LG Node: Query Transform---\")\n",
    "    # state[\"original_question\"] should be set when invoking the graph\n",
    "    state[\"transformed_question\"] = state[\"original_question\"]\n",
    "    return state\n",
    "\n",
    "# Node 2: Document Retrieval\n",
    "def retrieve_documents_node(state: AdvancedRAGState) -> AdvancedRAGState:\n",
    "    print(\"---LG Node: Retrieve Documents---\")\n",
    "    query = state[\"transformed_question\"]\n",
    "    \n",
    "    # Access global retrievers (ensure they are created and populated before running graph)\n",
    "    if 'text_retriever' in globals() and text_retriever:\n",
    "        state[\"retrieved_text_docs\"] = text_retriever.invoke(query)\n",
    "        print(f\"Retrieved {len(state['retrieved_text_docs'])} text documents.\")\n",
    "    else:\n",
    "        state[\"retrieved_text_docs\"] = []\n",
    "        print(\"Warning: Global `text_retriever` not found or not initialized.\")\n",
    "        \n",
    "    if 'image_desc_retriever' in globals() and image_desc_retriever:\n",
    "        state[\"retrieved_image_desc_docs\"] = image_desc_retriever.invoke(query)\n",
    "        print(f\"Retrieved {len(state['retrieved_image_desc_docs'])} image description documents.\")\n",
    "    else:\n",
    "        state[\"retrieved_image_desc_docs\"] = []\n",
    "        print(\"Warning: Global `image_desc_retriever` not found or not initialized.\")\n",
    "    return state\n",
    "\n",
    "# Node 3: Rerank and Select Context\n",
    "def rerank_and_select_node(state: AdvancedRAGState) -> AdvancedRAGState:\n",
    "    print(\"---LG Node: Rerank and Select---\")\n",
    "    retrieved_texts = state.get(\"retrieved_text_docs\", [])\n",
    "    retrieved_image_descs = state.get(\"retrieved_image_desc_docs\", [])\n",
    "\n",
    "    # Using k values from your notebook's retriever setup (or define desired final k here)\n",
    "    # Your notebook used k=10 for text retriever, k=8 for image desc retriever.\n",
    "    # And then selected k_txt=7, k_img=5 for final context.\n",
    "    k_text_final = 7\n",
    "    k_images_final = 5\n",
    "\n",
    "    # Prioritize critical documents (corpus summary, title, abstract)\n",
    "    final_text_selection = []\n",
    "    critical_content_prefixes_seen = set() # To avoid adding very similar critical docs\n",
    "\n",
    "    # Add corpus summary first if available (assuming it's important)\n",
    "    # The corpus summary is already prepended in all_extracted_elements, so it should be retrieved if relevant.\n",
    "    # We can specifically look for it in retrieved_texts or assume retriever handles its importance.\n",
    "    \n",
    "    # Simple prioritization: critical docs first, then others, up to k_text_final\n",
    "    # This logic matches your notebook's rerank_and_select_node\n",
    "    for doc in retrieved_texts:\n",
    "        if doc.metadata.get(\"importance\") == \"critical\":\n",
    "            prefix = doc.page_content[:200]\n",
    "            if prefix not in critical_content_prefixes_seen:\n",
    "                final_text_selection.append(doc)\n",
    "                critical_content_prefixes_seen.add(prefix)\n",
    "    \n",
    "    for doc in retrieved_texts:\n",
    "        if doc.metadata.get(\"importance\") != \"critical\":\n",
    "            prefix = doc.page_content[:200]\n",
    "            is_already_selected = any(sel_doc.page_content[:200] == prefix for sel_doc in final_text_selection)\n",
    "            if not is_already_selected:\n",
    "                final_text_selection.append(doc)\n",
    "    \n",
    "    state[\"final_text_context\"] = final_text_selection[:k_text_final]\n",
    "    state[\"final_image_context\"] = retrieved_image_descs[:k_images_final]\n",
    "    print(f\"Selected {len(state['final_text_context'])} text and {len(state['final_image_context'])} image description contexts.\")\n",
    "\n",
    "    # Prepare image payload for LLM (if model can handle direct image input)\n",
    "    # MAX_IMAGES_TO_LLM_FINAL is from Cell 2 config\n",
    "    images_payload = []\n",
    "    for img_desc_doc in state[\"final_image_context\"][:MAX_IMAGES_TO_LLM_FINAL]:\n",
    "        image_path = img_desc_doc.metadata.get(\"image_path\") # Path to the rendered image\n",
    "        if image_path and os.path.exists(image_path):\n",
    "            b64_str, mime = image_to_base64(image_path) # Uses helper from Cell 4.1\n",
    "            if b64_str and mime:\n",
    "                images_payload.append({\n",
    "                    \"path\": image_path,\n",
    "                    \"vlm_description\": img_desc_doc.page_content, # The VLM desc of this image\n",
    "                    \"data\": b64_str,\n",
    "                    \"mime_type\": mime\n",
    "                })\n",
    "    state[\"images_for_llm_payload\"] = images_payload\n",
    "    print(f\"Prepared {len(images_payload)} actual images for LLM payload (if model supports).\")\n",
    "    return state\n",
    "\n",
    "# Node 4: Generate Answer\n",
    "def generate_answer_advanced_node(state: AdvancedRAGState) -> AdvancedRAGState:\n",
    "    print(\"---LG Node: Generate Answer---\")\n",
    "    question = state[\"original_question\"]\n",
    "    text_ctx_str = format_docs_for_llm(state.get(\"final_text_context\", []), \"textual\")\n",
    "    img_desc_ctx_str = format_docs_for_llm(state.get(\"final_image_context\", []), \"image descriptions\")\n",
    "    actual_images_for_llm = state.get(\"images_for_llm_payload\", [])\n",
    "    \n",
    "    # Access global overall_corpus_summary (string, created by extract_elements_from_all_pdfs_advanced)\n",
    "    # Ensure 'overall_corpus_summary' is in the global scope and populated.\n",
    "    corpus_summary_context_str = \"No global corpus summary available for this run.\"\n",
    "    if 'overall_corpus_summary' in globals() and overall_corpus_summary:\n",
    "        corpus_summary_context_str = f\"--- OVERALL CORPUS SUMMARY ---\\n{overall_corpus_summary}\\n--- END CORPUS SUMMARY ---\"\n",
    "\n",
    "\n",
    "    # System prompt (adapted from your notebook's generate_answer_advanced_node)\n",
    "    system_prompt = (\n",
    "        \"You are an expert AI research assistant. Your task is to provide comprehensive, accurate, \"\n",
    "        \"and well-structured answers to user questions based *only* on the provided documents as context.\\n\\n\"\n",
    "        \"**Context Prioritization:** Prioritize context explicitly labeled 'DOCUMENT TITLE:', 'DOCUMENT ABSTRACT:', or 'CORPUS SUMMARY:'.\\n\\n\"\n",
    "        \"**Figure/Table Description:**\\n\"\n",
    "        \"- When asked to 'describe' a specific figure/table (e.g., 'Figure 1', 'Table 2'):\\n\"\n",
    "        \"    - **Prioritize 'image_description' documents or 'text_table_content' that have a matching 'caption_id'.** Consolidate if multiple parts exist.\\n\"\n",
    "        \"    - **If no direct VLM/image description is found but a 'text_figure_description' (caption text) is, describe based on that text.**\\n\"\n",
    "        \"    - **For visual descriptions from images, explain key components, structure, text within, and purpose.**\\n\\n\"\n",
    "        \"**General Guidance:**\\n\"\n",
    "        \"- Maintain a neutral, informative tone.\\n\"\n",
    "        \"- If information is not found in the provided context, state that clearly.\\n\"\n",
    "        \"- Format your answer clearly (headings, bullets).\\n\"\n",
    "        \"- Cite page numbers (P:X) or source document names (S:filename) if available in metadata. Mention caption_id (e.g., Figure 1) if relevant.\\n\"\n",
    "        \"- You might be given context from multiple unrelated documents. Identify relevant pieces yourself.\"\n",
    "    )\n",
    "\n",
    "    human_message_content_parts = []\n",
    "    human_message_content_parts.append({\"type\": \"text\", \"text\": corpus_summary_context_str})\n",
    "    human_message_content_parts.append({\"type\": \"text\", \"text\": f\"\\n\\n--- TEXTUAL & TEXTUAL TABLE CONTEXT ---\\n{text_ctx_str}\\n--- END TEXTUAL CONTEXT ---\"})\n",
    "    human_message_content_parts.append({\"type\": \"text\", \"text\": f\"\\n\\n--- VLM DESCRIPTIONS OF VISUAL ELEMENTS ---\\n{img_desc_ctx_str}\\n--- END VLM DESCRIPTIONS ---\"})\n",
    "\n",
    "    if actual_images_for_llm:\n",
    "        human_message_content_parts.append({\"type\": \"text\", \"text\": f\"\\n\\n--- ({len(actual_images_for_llm)}) ACTUAL IMAGES ATTACHED FOR REFERENCE ---\"})\n",
    "        for i, img_data in enumerate(actual_images_for_llm):\n",
    "            human_message_content_parts.append({\"type\": \"text\", \"text\": f\"Attached Image {i+1} Ref ({os.path.basename(img_data['path'])}). Summary of its VLM Desc: '{img_data['vlm_description'][:100]}...'\"})\n",
    "            human_message_content_parts.append({\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:{img_data['mime_type']};base64,{img_data['data']}\"}})\n",
    "        human_message_content_parts.append({\"type\": \"text\", \"text\": \"--- END ACTUAL IMAGES ---\"})\n",
    "    else:\n",
    "        human_message_content_parts.append({\"type\": \"text\", \"text\": \"\\n\\nNo actual images attached to this query for the model.\"})\n",
    "\n",
    "    human_message_content_parts.append({\"type\": \"text\", \"text\": f\"\\n\\n--- USER QUESTION ---\\n{question}\\n\\n--- ANSWER (Based *only* on provided context) ---\"})\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=human_message_content_parts)\n",
    "    ]\n",
    "\n",
    "    final_answer = \"Error: RAG LLM not available or invocation failed.\"\n",
    "    if 'llm_rag' in globals() and llm_rag: # Ensure llm_rag is defined\n",
    "        try:\n",
    "            response = llm_rag.invoke(messages)\n",
    "            final_answer = response.content\n",
    "        except Exception as e:\n",
    "            final_answer = f\"LLM generation error: {e}\"\n",
    "            print(f\"Error during LLM answer generation: {e}\")\n",
    "            # import traceback; traceback.print_exc()\n",
    "    else:\n",
    "        print(\"Warning: Global `llm_rag` not found for answer generation.\")\n",
    "        \n",
    "    state[\"answer\"] = final_answer\n",
    "    return state\n",
    "\n",
    "print(\"LangGraph nodes defined.\")\n",
    "\n",
    "# --- 4. Build and Compile the LangGraph ---\n",
    "workflow = StateGraph(AdvancedRAGState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"query_transformer\", query_transform_node)\n",
    "workflow.add_node(\"document_retriever\", retrieve_documents_node)\n",
    "workflow.add_node(\"context_selector\", rerank_and_select_node)\n",
    "workflow.add_node(\"answer_generator\", generate_answer_advanced_node)\n",
    "\n",
    "# Define edges\n",
    "workflow.set_entry_point(\"query_transformer\")\n",
    "workflow.add_edge(\"query_transformer\", \"document_retriever\")\n",
    "workflow.add_edge(\"document_retriever\", \"context_selector\")\n",
    "workflow.add_edge(\"context_selector\", \"answer_generator\")\n",
    "workflow.add_edge(\"answer_generator\", END) # END is from langgraph.graph\n",
    "\n",
    "# Compile the graph\n",
    "app_advanced_rag = workflow.compile()\n",
    "print(\"LangGraph RAG agent compiled successfully as 'app_advanced_rag'.\")\n",
    "\n",
    "# Optional: Visualize the graph\n",
    "# This requires graphviz to be installed (pip install graphviz, and system install: sudo apt-get install graphviz)\n",
    "# Also, ensure RUN_ID_FULL_EXTRACTION is defined from the previous cell if you use it in the path.\n",
    "# If not, use a generic name or remove RUN_ID from path.\n",
    "# For simplicity, let's use a fixed name for the graph image in the notebook.\n",
    "graph_image_save_path = os.path.join(EXPLORATION_OUTPUT_BASE, \"vtierp_langgraph.png\")\n",
    "try:\n",
    "    # Ensure the directory for the image exists\n",
    "    os.makedirs(os.path.dirname(graph_image_save_path), exist_ok=True)\n",
    "    \n",
    "    graph_png_data = app_advanced_rag.get_graph().draw_png()\n",
    "    with open(graph_image_save_path, \"wb\") as f:\n",
    "        f.write(graph_png_data)\n",
    "    print(f\"\\nLangGraph visualization saved to: {graph_image_save_path}\")\n",
    "    \n",
    "    # To display in notebook (if IPython is available)\n",
    "    # from IPython.display import Image as IPImage, display\n",
    "    # display(IPImage(filename=graph_image_save_path))\n",
    "except ImportError:\n",
    "    print(\"Skipping graph visualization display in notebook (IPython not available).\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not draw or save LangGraph graph (is graphviz installed and in PATH?): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Testing the VTIERP Agent\n",
    "\n",
    "Let's test the agent with a variety of queries, including those that require understanding visual context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Test Queries for Advanced VTIERP RAG Agent ('app_advanced_rag') ---\n",
      "\n",
      "\n",
      "--- Test Query 1 ---\n",
      "QUESTION: What is the full form of AAG and what does its abstract say about its purpose?\n",
      "---LG Node: Query Transform---\n",
      "---LG Node: Retrieve Documents---\n",
      "Retrieved 10 text documents.\n",
      "Retrieved 8 image description documents.\n",
      "---LG Node: Rerank and Select---\n",
      "Selected 7 text and 5 image description contexts.\n",
      "Prepared 3 actual images for LLM payload (if model supports).\n",
      "---LG Node: Generate Answer---\n",
      "\n",
      "ANSWER:\n",
      "The full form of AAG is **Analogy-Augmented Generation** (S:corpus_summary_nb).\n",
      "\n",
      "Based on the overall corpus summary, AAG's purpose is to leverage procedural memory and past experiences for procedural question-answering (Q&A) and for generating novel outputs, such as recipes (S:corpus_summary_nb).\n",
      "\n",
      "\n",
      "--- Test Query 2 ---\n",
      "QUESTION: Describe Figure 1 in AAG.pdf visually and explain its significance based on surrounding text or VLM description.\n",
      "---LG Node: Query Transform---\n",
      "---LG Node: Retrieve Documents---\n",
      "Retrieved 10 text documents.\n",
      "Retrieved 8 image description documents.\n",
      "---LG Node: Rerank and Select---\n",
      "Selected 7 text and 5 image description contexts.\n",
      "Prepared 3 actual images for LLM payload (if model supports).\n",
      "---LG Node: Generate Answer---\n",
      "\n",
      "ANSWER:\n",
      "Based on the provided documents, there is no information available to describe \"Figure 1 in AAG.pdf\" visually or explain its significance. The context mentions Figure 3 and Figure 4 within AAG.pdf, but not Figure 1.\n",
      "\n",
      "The \"Figure 1\" that appears in the VLM descriptions and attached images (page1_SEMANTIC_Figure1.png) is associated with a \"prompt optimization pipeline\" and appears to be from the \"Self_rewarding_VLLM.pdf\" document, not AAG.pdf.\n",
      "\n",
      "\n",
      "--- Test Query 3 ---\n",
      "QUESTION: Summarize the data presented in any tables on page 6 of AAG.pdf. What are the key column headers and data trends?\n",
      "---LG Node: Query Transform---\n",
      "---LG Node: Retrieve Documents---\n",
      "Retrieved 10 text documents.\n",
      "Retrieved 8 image description documents.\n",
      "---LG Node: Rerank and Select---\n",
      "Selected 7 text and 5 image description contexts.\n",
      "Prepared 3 actual images for LLM payload (if model supports).\n",
      "---LG Node: Generate Answer---\n",
      "\n",
      "ANSWER:\n",
      "On page 6 of AAG.pdf, the primary table discussed is **Table 1: Results of the LLM-based pairwise evaluation**.\n",
      "\n",
      "**Key Column Headers:**\n",
      "*   **Dataset:** This column specifies the dataset used for evaluation, which includes \"LCStep\" and \"RecipeNLG\".\n",
      "*   **Method:** This column lists the different methods being compared against AAG. These include baselines (Zero-Shot, Few Shot, RAG, ReAct) and ablations of AAG (NO QG, NO AG, NO CR, NO AG - NO CR).\n",
      "*   **AAG win:** This indicates the percentage of times the Language Model (LLM) chose AAG over the other method in the pairwise comparison.\n",
      "*   **AAG loss:** This indicates the percentage of times the LLM chose the other method over AAG.\n",
      "*   **Ties:** This indicates the percentage of times the LLM was indifferent to the two options being compared.\n",
      "\n",
      "**Data Trends:**\n",
      "*   **Overall Preference for AAG:** The LLM substantially prefers the procedures generated by the AAG system over all other baselines, with a margin greater than 40% (P:6).\n",
      "*   **Baseline Comparisons:** For the first four rows, which compare AAG with baselines, AAG consistently shows high \"AAG win\" percentages. For example, against ReAct, AAG wins 98.21% on LCStep and 97.65% on RecipeNLG, with very low \"AAG loss\" and \"Ties\" percentages (P:6).\n",
      "*   **Ablation Studies:** The last four rows correspond to different ablations of AAG. AAG generally performs better than any of its ablation variants, highlighting the importance of each component in its pipeline (P:6, P:7). Specifically, the numbers for the AAG-NOQG variant underscore the significance of analogy in the AAG system (P:7).\n",
      "*   **Pairwise Nature:** It is important to note that the reported numbers are from a pairwise comparison, meaning they cannot be compared across different methods. A higher number for a technique signifies that the LLM is more confident that one system is better than the other *in that specific pair*, not necessarily better than other systems not in the pair (P:6).\n",
      "*   **Percentage Reporting:** All reported numbers in the table are in percentage (P:6).\n",
      "\n",
      "\n",
      "--- Test Query 4 ---\n",
      "QUESTION: What methodology is shown in any diagrams within the first few pages of AAG.pdf?\n",
      "---LG Node: Query Transform---\n",
      "---LG Node: Retrieve Documents---\n",
      "Retrieved 10 text documents.\n",
      "Retrieved 8 image description documents.\n",
      "---LG Node: Rerank and Select---\n",
      "Selected 7 text and 5 image description contexts.\n",
      "Prepared 3 actual images for LLM payload (if model supports).\n",
      "---LG Node: Generate Answer---\n",
      "\n",
      "ANSWER:\n",
      "Based on the provided context, the AAG.pdf document refers to \"Figure 2\" as depicting the detailed workflow of the Analogy-Augmented Generation (AAG) system (S:AAG.pdf, P:5).\n",
      "\n",
      "The methodology described for the AAG system's workflow (as referenced by Figure 2 in AAG.pdf) involves:\n",
      "*   Taking a user query in the format \"<output> using <input>\", specifying the goal and available resources.\n",
      "*   A RAG (Retrieval-Augmented Generation) module searching a procedural memory to retrieve the top K=3 relevant procedures based on cosine similarity.\n",
      "*   A query generator module breaking the input query into N parts.\n",
      "*   An iterative refinement process where an LLM critic re-evaluates edited procedures for a maximum of T=3 cycles, with the self-refined version returned as the final output (S:AAG.pdf, P:4, P:5).\n",
      "\n",
      "However, the provided VLM description for \"Figure 2\" (Source Hint: `page3_SEMANTIC_Figure2.png`, Page: 3) describes \"The overall framework in our prompt optimizing framework,\" which involves components like \"Stable Diffusion\" and \"VILA-v1.5-8b\" for image generation and prompt optimization. This description pertains to a different methodology than the AAG system's workflow.\n",
      "\n",
      "An uncaptioned visual element (`p3_draw0`) is also present on page 4 of AAG.pdf, but its VLM description was skipped and is therefore not available in the provided context.\n",
      "\n",
      "\n",
      "--- Test Query 5 ---\n",
      "QUESTION: Describe me what does the LCStep MEANT IN THIS research paper \n",
      "---LG Node: Query Transform---\n",
      "---LG Node: Retrieve Documents---\n",
      "Retrieved 10 text documents.\n",
      "Retrieved 8 image description documents.\n",
      "---LG Node: Rerank and Select---\n",
      "Selected 7 text and 5 image description contexts.\n",
      "Prepared 3 actual images for LLM payload (if model supports).\n",
      "---LG Node: Generate Answer---\n",
      "\n",
      "ANSWER:\n",
      "In this research paper, **LCStep** refers to a novel dataset specifically designed for evaluating procedural knowledge systems (S:AAG.pdf, P:2).\n",
      "\n",
      "Here's what LCStep means in this context:\n",
      "\n",
      "*   **Purpose:** It consists of procedures for performing tasks using the LangChain Python library (S:AAG.pdf, P:2). Its recency allows for testing procedural knowledge systems on topics that might not be present in some Large Language Models' (LLMs) training data (S:AAG.pdf, P:2). It is considered an \"unfamiliar domain\" for evaluation (S:AAG.pdf, P:6).\n",
      "*   **Content:** The LCStep dataset contains three sets of documents: API references, conceptual documentation, and procedures. For the purpose of this paper, only the procedures were utilized (S:AAG.pdf, P:12).\n",
      "*   **Collection:**\n",
      "    *   It was collected from 180 tutorial pages found in the Python section of the LangChain website (S:AAG.pdf, P:3).\n",
      "    *   An LLM-enabled pipeline, combined with human oversight and quality review, was used to extract 276 procedures from these tutorials (S:AAG.pdf, P:3, P:6).\n",
      "    *   The process of generating the LCStep data is illustrated in Figure 4 (S:AAG.pdf, P:12, P:13).\n",
      "*   **Role in the Research:** LCStep serves as a key dataset for evaluating the Analogy-Augmented Generation (AAG) system, alongside RecipeNLG (S:AAG.pdf, P:5). The paper evaluates the AAG system on this unfamiliar domain (S:AAG.pdf, P:2).\n",
      "\n",
      "\n",
      "--- Test Query 6 ---\n",
      "QUESTION: Analyze the Table 1 in details and then summerize what does the data inside this table trying to convey\n",
      "---LG Node: Query Transform---\n",
      "---LG Node: Retrieve Documents---\n",
      "Retrieved 10 text documents.\n",
      "Retrieved 8 image description documents.\n",
      "---LG Node: Rerank and Select---\n",
      "Selected 7 text and 5 image description contexts.\n",
      "Prepared 3 actual images for LLM payload (if model supports).\n",
      "---LG Node: Generate Answer---\n",
      "\n",
      "ANSWER:\n",
      "I am sorry, but the provided context does not contain any information about \"Table 1\". Therefore, I cannot analyze it or summarize its data.\n",
      "\n",
      "\n",
      "--- Test Query 7 ---\n",
      "QUESTION: Replicate or visualize show both table 1 and table 2 , exactly how it's on the paper.\n",
      "---LG Node: Query Transform---\n",
      "---LG Node: Retrieve Documents---\n",
      "Retrieved 10 text documents.\n",
      "Retrieved 8 image description documents.\n",
      "---LG Node: Rerank and Select---\n",
      "Selected 7 text and 5 image description contexts.\n",
      "Prepared 3 actual images for LLM payload (if model supports).\n",
      "---LG Node: Generate Answer---\n",
      "\n",
      "ANSWER:\n",
      "Based on the provided documents, neither \"Table 1\" nor \"Table 2\" are present. The context includes descriptions and images for \"Table 3\" (which is a pie chart visualization), \"Figure 1\", and \"Figure 2\", but not the specific tables requested.\n",
      "\n",
      "\n",
      "--- Test Query 8 ---\n",
      "QUESTION: What's difference between Table 1 and Table 2?\n",
      "---LG Node: Query Transform---\n",
      "---LG Node: Retrieve Documents---\n",
      "Retrieved 10 text documents.\n",
      "Retrieved 8 image description documents.\n",
      "---LG Node: Rerank and Select---\n",
      "Selected 7 text and 5 image description contexts.\n",
      "Prepared 3 actual images for LLM payload (if model supports).\n",
      "---LG Node: Generate Answer---\n",
      "\n",
      "ANSWER:\n",
      "Based on the provided context:\n",
      "\n",
      "**Table 2** is described as \"Results of the LLM-based pairwise evaluation\" (S: AAG.pdf, P: 13). It presents a comparison of Analogy-Augmented Generation (AAG) with other methods and ablations. The table shows percentages for \"AAG win\" (LLM chose AAG), \"AAG loss\" (LLM chose the other method), and \"Ties\" (LLM was indifferent). It includes results for methods such as Zero-Shot, Few Shot, RAG, ReAct, NO QG, NO AG, NO CR, and NO AG - NO CR (S: AAG.pdf, P: 13).\n",
      "\n",
      "The provided context **does not contain any information about Table 1**.\n",
      "\n",
      "\n",
      "--- Test Query 9 ---\n",
      "QUESTION: Explain figure 4 in detail\n",
      "---LG Node: Query Transform---\n",
      "---LG Node: Retrieve Documents---\n",
      "Retrieved 10 text documents.\n",
      "Retrieved 8 image description documents.\n",
      "---LG Node: Rerank and Select---\n",
      "Selected 7 text and 5 image description contexts.\n",
      "Prepared 3 actual images for LLM payload (if model supports).\n",
      "---LG Node: Generate Answer---\n",
      "\n",
      "ANSWER:\n",
      "Based on the provided context, \"Figure 4\" is described by the caption: \"The workflow used to generate the LCStep dataset.\" (S: page13_SEMANTIC_Figure4.png, page7_SEMANTIC_Figure4.png).\n",
      "\n",
      "Visually, the image corresponding to this caption (page13_SEMANTIC_Figure4.png) depicts a flowchart illustrating a multi-step process for generating \"procedures\" and \"API reference\" documentation.\n",
      "\n",
      "The workflow begins with:\n",
      "*   **\"scraped docs from LangChain\"**: This input leads to a \"Human labeler\" who processes \"tutorials/guides\" to feed into \"GPT-4 procedure extraction\".\n",
      "*   **\"API reference docs generated with Sphinx\"**: This input directly leads to \"API reference\" (highlighted in red).\n",
      "\n",
      "The main procedural flow involves:\n",
      "1.  **\"GPT-4 procedure extraction\"**: This step takes input from the human-labeled tutorials/guides.\n",
      "2.  **\"GPT-4 procedure validation\"**: The output from extraction is then validated.\n",
      "    *   If validation \"pass\"es, it moves to \"human approval\".\n",
      "    *   If validation \"fail\"s, it goes to \"GPT-4 revision\".\n",
      "3.  **\"GPT-4 revision\"**: After revision, the process moves to \"human revision\".\n",
      "4.  **\"human revision\"**: This step, along with \"human approval\", both lead to the final \"procedures\" (highlighted in red).\n",
      "\n",
      "Additionally, the \"Human labeler\" also processes \"concept docs\" which directly lead to \"concept docs\" (highlighted in red), indicating another output or type of documentation.\n",
      "\n",
      "The diagram uses rectangles for processes/steps, rounded rectangles for inputs/outputs, and arrows to show the flow of information and decisions.\n",
      "\n",
      "(Note: The detailed VLM description for this figure was skipped due to character limits in the provided context.)\n",
      "\n",
      "\n",
      "--- Test Query 10 ---\n",
      "QUESTION: How many different research papers have I uploaded to you for processing in this corpus?\n",
      "---LG Node: Query Transform---\n",
      "---LG Node: Retrieve Documents---\n",
      "Retrieved 10 text documents.\n",
      "Retrieved 8 image description documents.\n",
      "---LG Node: Rerank and Select---\n",
      "Selected 7 text and 5 image description contexts.\n",
      "Prepared 3 actual images for LLM payload (if model supports).\n",
      "---LG Node: Generate Answer---\n",
      "\n",
      "ANSWER:\n",
      "Based on the provided context, there are two different research papers:\n",
      "\n",
      "1.  **AAG.pdf** (related to Analogy-Augmented Generation)\n",
      "2.  **Self_rewarding_VLLM.pdf** (related to Self-Rewarding Large Vision-Language Models)\n",
      "\n",
      "\n",
      "--- Test Query 11 ---\n",
      "QUESTION: How are the different papers in this corpus (AAG.pdf and Self_rewarding_VLLM.pdf) similar and different in their core ideas?\n",
      "---LG Node: Query Transform---\n",
      "---LG Node: Retrieve Documents---\n",
      "Retrieved 10 text documents.\n",
      "Retrieved 8 image description documents.\n",
      "---LG Node: Rerank and Select---\n",
      "Selected 7 text and 5 image description contexts.\n",
      "Prepared 3 actual images for LLM payload (if model supports).\n",
      "---LG Node: Generate Answer---\n",
      "\n",
      "ANSWER:\n",
      "The two papers, `AAG.pdf` (Analogy-Augmented Generation) and `Self_rewarding_VLLM.pdf` (Self-Rewarding Large Vision-Language Models), share similarities in their overarching goals as advanced AI systems but differ significantly in their specific applications and core mechanisms.\n",
      "\n",
      "**Similarities:**\n",
      "*   **Generative Tasks and Problem-Solving:** Both systems are designed for **generative tasks** and **problem-solving** within the domain of AI (OVERALL CORPUS SUMMARY, S:corpus_summary_nb).\n",
      "*   **Self-Improvement/Refinement:** Both incorporate mechanisms for self-improvement or refinement:\n",
      "    *   **AAG** utilizes an iterative process involving an \"LLM Critic\" and an \"LLM Edit Performer\" to revise and refine procedures until they are deemed satisfactory (Figure 2, P:4).\n",
      "    *   **Self-Rewarding VLLM** employs a \"self-rewarding prompt rewriting model\" where the model itself generates rewards for its outputs, enabling AI self-feedback and an evolving training process to optimize prompts (S:Self_rewarding_VLLM.pdf, P:4, 7).\n",
      "*   **Leveraging Large Language Models (LLMs):** Both systems fundamentally rely on Large Language Models. AAG explicitly uses LLMs for analogical reasoning and procedural generation (Figure 1, P:1), and its evaluation involves LLM-based pairwise comparisons (Table 1, P:6). Self-Rewarding VLLM, as a \"Large Vision-Language Model,\" also integrates LLM capabilities.\n",
      "\n",
      "**Differences:**\n",
      "*   **Primary Application/Domain:**\n",
      "    *   **AAG** focuses on **procedural Q&A** and generating **novel procedural outputs**, such as recipes, by leveraging procedural memory and past experiences (OVERALL CORPUS SUMMARY, S:corpus_summary_nb; Figure 1, P:1).\n",
      "    *   **Self-Rewarding VLLM** is designed to **optimize prompts for sophisticated text-to-image generation** (OVERALL CORPUS SUMMARY, S:corpus_summary_nb). Its goal is to improve the quality of images generated from text prompts.\n",
      "*   **Mechanism of Knowledge/Improvement:**\n",
      "    *   **AAG** operates by retrieving **analogical examples** from a \"Procedural Memory Database\" when faced with an unfamiliar problem. It then feeds these retrieved, structured examples to the LLM to guide the generation of new procedures (Figure 1, P:1).\n",
      "    *   **Self-Rewarding VLLM** improves by using the **model of the current iteration to generate rewards** for its own outputs (e.g., scoring generated images based on rewritten prompts). This allows the model to iteratively refine and optimize prompts through self-feedback, rather than relying on a fixed reward model or external human-labeled data (S:Self_rewarding_VLLM.pdf, P:4, 7).\n",
      "*   **Output Focus:**\n",
      "    *   **AAG** directly generates **procedural steps or recipes** (Figure 1, P:1).\n",
      "    *   **Self-Rewarding VLLM** primarily outputs **optimized prompts** that are then used by a text-to-image model to generate images (S:Self_rewarding_VLLM.pdf, P:7).\n",
      "\n",
      "\n",
      "--- All Test Queries for LangGraph Agent Finished ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Testing the Advanced VTIERP RAG Agent (Notebook Version)\n",
    "\n",
    "# Ensure AdvancedRAGState is defined (it should be from the previous LangGraph cell)\n",
    "if 'AdvancedRAGState' not in globals():\n",
    "    print(\"Error: AdvancedRAGState TypedDict is not defined. Please run the LangGraph definition cell.\")\n",
    "    # You might want to stop execution here or define it again if necessary\n",
    "    # For example:\n",
    "    # from typing import List, Dict, TypedDict, Optional, Any, Set\n",
    "    # class AdvancedRAGState(TypedDict):\n",
    "    #     original_question: str\n",
    "    #     transformed_question: str\n",
    "    #     retrieved_text_docs: List[Document]\n",
    "    #     retrieved_image_desc_docs: List[Document]\n",
    "    #     final_text_context: List[Document]\n",
    "    #     final_image_context: List[Document]\n",
    "    #     images_for_llm_payload: List[Dict[str, Any]]\n",
    "    #     answer: str\n",
    "\n",
    "\n",
    "# Ensure the compiled LangGraph app is available\n",
    "if 'app_advanced_rag' not in globals():\n",
    "    print(\"Error: `app_advanced_rag` (compiled LangGraph) is not defined. Please run the LangGraph definition and compilation cell.\")\n",
    "    # Stop execution or handle\n",
    "    raise NameError(\"`app_advanced_rag` is not defined.\")\n",
    "\n",
    "# Ensure overall_corpus_summary is available\n",
    "if 'overall_corpus_summary' not in globals():\n",
    "    print(\"Warning: `overall_corpus_summary` is not defined globally. The agent might not have corpus-level summary context for prompts.\")\n",
    "    # Set a default if it's missing, though it's better if the previous cell populates it.\n",
    "    overall_corpus_summary = \"Corpus summary not available for this test run.\"\n",
    "\n",
    "\n",
    "# --- Define Test Queries (same as in your original notebook) ---\n",
    "test_queries_adv = [\n",
    "    \"What is the full form of AAG and what does its abstract say about its purpose?\",\n",
    "    \"Describe Figure 1 in AAG.pdf visually and explain its significance based on surrounding text or VLM description.\",\n",
    "    \"Summarize the data presented in any tables on page 6 of AAG.pdf. What are the key column headers and data trends?\", # Your notebook had \"Table 1 on page 6\"\n",
    "    \"What methodology is shown in any diagrams within the first few pages of AAG.pdf?\",\n",
    "    \"Describe me what does the LCStep MEANT IN THIS research paper \",\n",
    "    \"Analyze the Table 1 in details and then summerize what does the data inside this table trying to convey\", # This query might still fail if Table 1 isn't explicitly found/described as such\n",
    "    \"Replicate or visualize show both table 1 and table 2 , exactly how it's on the paper.\", # This might be hard for LLM without explicit table data matching the exact name\n",
    "    \"What's difference between Table 1 and Table 2?\",\n",
    "    \"Explain figure 4 in detail\",\n",
    "    \"How many different research papers have I uploaded to you for processing in this corpus?\", # Adjusted for clarity\n",
    "    \"How are the different papers in this corpus (AAG.pdf and Self_rewarding_VLLM.pdf) similar and different in their core ideas?\" # Adjusted for clarity\n",
    "]\n",
    "\n",
    "print(f\"\\n--- Starting Test Queries for Advanced VTIERP RAG Agent ('app_advanced_rag') ---\")\n",
    "\n",
    "for i, query in enumerate(test_queries_adv):\n",
    "    print(f\"\\n\\n--- Test Query {i+1} ---\")\n",
    "    print(f\"QUESTION: {query}\")\n",
    "\n",
    "    # Prepare the initial input dictionary for the LangGraph state.\n",
    "    # All keys from AdvancedRAGState must be present.\n",
    "    # The LangGraph invoke method expects a dictionary that can initialize the state.\n",
    "    # The `overall_corpus_summary` is accessed globally by the nodes, not passed via state here.\n",
    "    \n",
    "    # Initialize state dictionary with default values for all keys in AdvancedRAGState\n",
    "    initial_input_state = {}\n",
    "    try:\n",
    "        for key_name, type_hint in AdvancedRAGState.__annotations__.items():\n",
    "            # Basic default value initialization based on common types\n",
    "            origin = getattr(type_hint, '__origin__', None)\n",
    "            if origin is list or str(type_hint).startswith(\"typing.List\") or str(type_hint).startswith(\"List[\"):\n",
    "                initial_input_state[key_name] = []\n",
    "            elif origin is dict or str(type_hint).startswith(\"typing.Dict\") or str(type_hint).startswith(\"Dict[\"):\n",
    "                initial_input_state[key_name] = {}\n",
    "            elif origin is Optional or (hasattr(type_hint, '__args__') and type(None) in type_hint.__args__): # Handles Optional[T]\n",
    "                initial_input_state[key_name] = None\n",
    "            elif type_hint is str:\n",
    "                initial_input_state[key_name] = \"\"\n",
    "            else: # Fallback for other types, though state usually sticks to these\n",
    "                initial_input_state[key_name] = None # Or raise an error if an unknown type\n",
    "    except NameError: # If AdvancedRAGState wasn't found initially\n",
    "        print(\"CRITICAL ERROR: AdvancedRAGState not defined. Cannot initialize input for LangGraph. Stopping tests.\")\n",
    "        break # Exit the loop\n",
    "\n",
    "    # Set the essential starting values\n",
    "    initial_input_state[\"original_question\"] = query\n",
    "    # transformed_question will be set by the first node\n",
    "    # other fields like retrieved_docs, answer will be populated by the graph\n",
    "\n",
    "    try:\n",
    "        # Invoke the compiled LangGraph application\n",
    "        # The 'config' dictionary can be used for things like recursion limits, thread pool executors, etc.\n",
    "        config = {\"recursion_limit\": 15} # As in your original notebook\n",
    "        \n",
    "        # Ensure `app_advanced_rag` is the correct compiled graph object\n",
    "        final_state_result = app_advanced_rag.invoke(initial_input_state, config=config)\n",
    "        \n",
    "        answer = final_state_result.get('answer', 'No answer was generated or found in the final state.')\n",
    "        print(f\"\\nANSWER:\\n{answer}\")\n",
    "\n",
    "        # Optional: Print some of the context used (for debugging)\n",
    "        # print(\"\\n--- Context Used (Sample) ---\")\n",
    "        # if final_state_result.get(\"final_text_context\"):\n",
    "        #     print(\"Text Context:\")\n",
    "        #     for doc_ctx in final_state_result[\"final_text_context\"][:2]: # Print first 2 text contexts\n",
    "        #         print(f\"  - Source: {doc_ctx.metadata.get('source', 'N/A')}, Page: {doc_ctx.metadata.get('page_number', 'N/A')}\")\n",
    "        #         print(f\"    Content: {doc_ctx.page_content[:150]}...\")\n",
    "        # if final_state_result.get(\"final_image_context\"):\n",
    "        #     print(\"Image Description Context:\")\n",
    "        #     for img_ctx_doc in final_state_result[\"final_image_context\"][:2]: # Print first 2 image contexts\n",
    "        #         print(f\"  - Caption ID: {img_ctx_doc.metadata.get('caption_id', 'N/A')}, Orig Caption: {img_ctx_doc.metadata.get('original_caption', 'N/A')[:70]}...\")\n",
    "        #         print(f\"    VLM Desc: {img_ctx_doc.page_content[:150]}...\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR during LangGraph execution for query '{query}':\")\n",
    "        print(f\"{e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() # Print full traceback for debugging the error\n",
    "\n",
    "print(\"\\n\\n--- All Test Queries for LangGraph Agent Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusion and Future Work\n",
    "\n",
    "This exploration demonstrates the feasibility and benefits of a hybrid multimodal RAG system for research papers. By extracting and describing visual elements alongside textual content, VTIERP can provide more comprehensive answers to complex queries. The use of PyMuPDF for detailed element extraction, combined with Gemini for VLM-like descriptions and LangGraph for agentic control, forms a powerful foundation.\n",
    "\n",
    "**Key Findings:**\n",
    "*   Semantic grouping of visual primitives with their captions is crucial.\n",
    "*   Generating textual descriptions for visuals (even simulated via VLM or good caption extraction) significantly enhances context for the LLM.\n",
    "*   Textual table content can be effectively extracted and treated as text.\n",
    "*   LangGraph provides excellent flexibility for orchestrating the multimodal RAG pipeline.\n",
    "\n",
    "**Future Work:**\n",
    "*   **True VLM Integration:** Replace simulated VLM descriptions with calls to actual advanced Vision-Language Models (e.g., Gemini Vision Pro directly for image understanding within the RAG chain if token limits/costs allow, or specialized image captioning/VQA models).\n",
    "*   **Improved Table Understanding:** Integrate more robust table parsing libraries (e.g., `unstructured.io` table extraction, or dedicated table-to-text models).\n",
    "*   **Advanced Query Decomposition:** For very complex queries, LangGraph could be used to decompose them into sub-queries for text and visual modalities.\n",
    "*   **Enhanced UI:** Develop a user-friendly interface (e.g., using Streamlit) to interact with the VTIERP system.\n",
    "*   **Scalability and Deployment:** Package the system into a deployable application (e.g., using FastAPI and Docker) following 12-Factor App principles.\n",
    "*   **Rigorous Evaluation:** Conduct a more formal evaluation with a larger set of documents and queries, comparing against baseline RAG systems.\n",
    "\n",
    "This notebook serves as the foundational research and experimentation leading to the development of the full VTIERP application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
